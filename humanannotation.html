<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Human annotation | Long-form recordings: From A to Z</title>
  <meta name="description" content="This bookdown contains the scripts of instructional videos created in the context of the ExELang Project (exelang.fr)." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Human annotation | Long-form recordings: From A to Z" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This bookdown contains the scripts of instructional videos created in the context of the ExELang Project (exelang.fr)." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Human annotation | Long-form recordings: From A to Z" />
  
  <meta name="twitter:description" content="This bookdown contains the scripts of instructional videos created in the context of the ExELang Project (exelang.fr)." />
  



<meta name="date" content="2021-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="accuracy.html"/>
<link rel="next" href="secondaryanalyses.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>




<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Long-form recordings: From A to Z</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="hardware.html"><a href="hardware.html"><i class="fa fa-check"></i><b>1</b> Hardware</a><ul>
<li class="chapter" data-level="1.1" data-path="hardware.html"><a href="hardware.html#lena-hardware"><i class="fa fa-check"></i><b>1.1</b> LENA hardware</a></li>
<li class="chapter" data-level="1.2" data-path="hardware.html"><a href="hardware.html#non-lena-hardware"><i class="fa fa-check"></i><b>1.2</b> Non-LENA hardware</a><ul>
<li class="chapter" data-level="1.2.1" data-path="hardware.html"><a href="hardware.html#ipods"><i class="fa fa-check"></i><b>1.2.1</b> iPods</a></li>
<li class="chapter" data-level="1.2.2" data-path="hardware.html"><a href="hardware.html#hand-held-recorders"><i class="fa fa-check"></i><b>1.2.2</b> Hand-held recorders</a></li>
<li class="chapter" data-level="1.2.3" data-path="hardware.html"><a href="hardware.html#usb-spy-recording-devices"><i class="fa fa-check"></i><b>1.2.3</b> USB “spy” recording devices</a></li>
<li class="chapter" data-level="1.2.4" data-path="hardware.html"><a href="hardware.html#babylogger"><i class="fa fa-check"></i><b>1.2.4</b> Babylogger</a></li>
<li class="chapter" data-level="1.2.5" data-path="hardware.html"><a href="hardware.html#wireless-systems"><i class="fa fa-check"></i><b>1.2.5</b> Wireless systems</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="hardware.html"><a href="hardware.html#the-bottom-line-regarding-hardware"><i class="fa fa-check"></i><b>1.3</b> The bottom line regarding hardware</a></li>
<li class="chapter" data-level="1.4" data-path="hardware.html"><a href="hardware.html#summary"><i class="fa fa-check"></i><b>1.4</b> Summary</a></li>
<li class="chapter" data-level="1.5" data-path="hardware.html"><a href="hardware.html#resources"><i class="fa fa-check"></i><b>1.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="add.html"><a href="add.html"><i class="fa fa-check"></i><b>2</b> Additional measures</a><ul>
<li class="chapter" data-level="2.1" data-path="add.html"><a href="add.html#movement"><i class="fa fa-check"></i><b>2.1</b> Movement</a></li>
<li class="chapter" data-level="2.2" data-path="add.html"><a href="add.html#visual-information-snapshots"><i class="fa fa-check"></i><b>2.2</b> Visual information: Snapshots</a></li>
<li class="chapter" data-level="2.3" data-path="add.html"><a href="add.html#visual-information-continuous-recordings"><i class="fa fa-check"></i><b>2.3</b> Visual information: Continuous recordings</a></li>
<li class="chapter" data-level="2.4" data-path="add.html"><a href="add.html#other-information"><i class="fa fa-check"></i><b>2.4</b> Other information</a></li>
<li class="chapter" data-level="2.5" data-path="add.html"><a href="add.html#resources-1"><i class="fa fa-check"></i><b>2.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="clothing.html"><a href="clothing.html"><i class="fa fa-check"></i><b>3</b> Clothing</a><ul>
<li class="chapter" data-level="3.1" data-path="clothing.html"><a href="clothing.html#custom-sized-pockets"><i class="fa fa-check"></i><b>3.1</b> Custom-sized pockets</a><ul>
<li class="chapter" data-level="3.1.1" data-path="clothing.html"><a href="clothing.html#lena-t-shirts"><i class="fa fa-check"></i><b>3.1.1</b> LENA t-shirts</a></li>
<li class="chapter" data-level="3.1.2" data-path="clothing.html"><a href="clothing.html#non-lena-t-shirts"><i class="fa fa-check"></i><b>3.1.2</b> Non-LENA t-shirts</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="clothing.html"><a href="clothing.html#alternatives-to-t-shirts"><i class="fa fa-check"></i><b>3.2</b> Alternatives to t-shirts</a><ul>
<li class="chapter" data-level="3.2.1" data-path="clothing.html"><a href="clothing.html#vests"><i class="fa fa-check"></i><b>3.2.1</b> Vests</a></li>
<li class="chapter" data-level="3.2.2" data-path="clothing.html"><a href="clothing.html#harness"><i class="fa fa-check"></i><b>3.2.2</b> Harness</a></li>
<li class="chapter" data-level="3.2.3" data-path="clothing.html"><a href="clothing.html#attaching-the-device-to-a-piece-of-clothing"><i class="fa fa-check"></i><b>3.2.3</b> Attaching the device to a piece of clothing</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="clothing.html"><a href="clothing.html#final-comments"><i class="fa fa-check"></i><b>3.3</b> Final comments</a></li>
<li class="chapter" data-level="3.4" data-path="clothing.html"><a href="clothing.html#resources-2"><i class="fa fa-check"></i><b>3.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="lena.html"><a href="lena.html"><i class="fa fa-check"></i><b>4</b> LENA software</a><ul>
<li class="chapter" data-level="4.1" data-path="lena.html"><a href="lena.html#resources-3"><i class="fa fa-check"></i><b>4.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nonlenasoftware.html"><a href="nonlenasoftware.html"><i class="fa fa-check"></i><b>5</b> Non-Lena Softwares</a><ul>
<li class="chapter" data-level="5.1" data-path="nonlenasoftware.html"><a href="nonlenasoftware.html#resources-4"><i class="fa fa-check"></i><b>5.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="childproject.html"><a href="childproject.html"><i class="fa fa-check"></i><b>6</b> Tutorial of our ChildProject software</a><ul>
<li class="chapter" data-level="6.1" data-path="childproject.html"><a href="childproject.html#resources-5"><i class="fa fa-check"></i><b>6.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="piloting.html"><a href="piloting.html"><i class="fa fa-check"></i><b>7</b> Piloting</a><ul>
<li class="chapter" data-level="7.1" data-path="piloting.html"><a href="piloting.html#resources-6"><i class="fa fa-check"></i><b>7.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="irb.html"><a href="irb.html"><i class="fa fa-check"></i><b>8</b> IRB</a><ul>
<li class="chapter" data-level="8.1" data-path="irb.html"><a href="irb.html#basic-information-for-all-irb-submissions"><i class="fa fa-check"></i><b>8.1</b> Basic information for all IRB submissions</a></li>
<li class="chapter" data-level="8.2" data-path="irb.html"><a href="irb.html#additional-considerations"><i class="fa fa-check"></i><b>8.2</b> Additional considerations</a></li>
<li class="chapter" data-level="8.3" data-path="irb.html"><a href="irb.html#text-you-can-use-in-your-irb-requests"><i class="fa fa-check"></i><b>8.3</b> Text you can use in your IRB requests:</a><ul>
<li class="chapter" data-level="8.3.1" data-path="irb.html"><a href="irb.html#basic-description"><i class="fa fa-check"></i><b>8.3.1</b> Basic description</a></li>
<li class="chapter" data-level="8.3.2" data-path="irb.html"><a href="irb.html#automated-analyses"><i class="fa fa-check"></i><b>8.3.2</b> Automated analyses</a></li>
<li class="chapter" data-level="8.3.3" data-path="irb.html"><a href="irb.html#manual-annotations-for-validation"><i class="fa fa-check"></i><b>8.3.3</b> Manual annotations for validation</a></li>
<li class="chapter" data-level="8.3.4" data-path="irb.html"><a href="irb.html#manual-annotations-to-augment-dataset"><i class="fa fa-check"></i><b>8.3.4</b> Manual annotations to augment dataset</a></li>
<li class="chapter" data-level="8.3.5" data-path="irb.html"><a href="irb.html#manual-annotations-using-zooniverse-general"><i class="fa fa-check"></i><b>8.3.5</b> Manual annotations using Zooniverse – general</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="irb.html"><a href="irb.html#closing-thoughts"><i class="fa fa-check"></i><b>8.4</b> Closing thoughts</a></li>
<li class="chapter" data-level="8.5" data-path="irb.html"><a href="irb.html#resources-7"><i class="fa fa-check"></i><b>8.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="logistics.html"><a href="logistics.html"><i class="fa fa-check"></i><b>9</b> Logistics</a><ul>
<li class="chapter" data-level="9.1" data-path="logistics.html"><a href="logistics.html#how-to-deliver-the-hardware"><i class="fa fa-check"></i><b>9.1</b> How to deliver the hardware</a></li>
<li class="chapter" data-level="9.2" data-path="logistics.html"><a href="logistics.html#how-long-to-record-for"><i class="fa fa-check"></i><b>9.2</b> How long to record for</a></li>
<li class="chapter" data-level="9.3" data-path="logistics.html"><a href="logistics.html#how-to-communicate-about-the-hardware-with-family-members"><i class="fa fa-check"></i><b>9.3</b> How to communicate about the hardware with family members</a></li>
<li class="chapter" data-level="9.4" data-path="logistics.html"><a href="logistics.html#how-many-devices-do-you-need"><i class="fa fa-check"></i><b>9.4</b> How many devices do you need?</a><ul>
<li class="chapter" data-level="9.4.1" data-path="logistics.html"><a href="logistics.html#sample-calculation-1"><i class="fa fa-check"></i><b>9.4.1</b> Sample calculation 1</a></li>
<li class="chapter" data-level="9.4.2" data-path="logistics.html"><a href="logistics.html#sample-calculation-2"><i class="fa fa-check"></i><b>9.4.2</b> Sample calculation 2</a></li>
<li class="chapter" data-level="9.4.3" data-path="logistics.html"><a href="logistics.html#sample-calculation-3"><i class="fa fa-check"></i><b>9.4.3</b> Sample calculation 3</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="logistics.html"><a href="logistics.html#resources-8"><i class="fa fa-check"></i><b>9.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="organizingdata.html"><a href="organizingdata.html"><i class="fa fa-check"></i><b>10</b> Organizing your data</a><ul>
<li class="chapter" data-level="10.1" data-path="organizingdata.html"><a href="organizingdata.html#what-information-to-collect-about-the-children"><i class="fa fa-check"></i><b>10.1</b> What information to collect about the children</a></li>
<li class="chapter" data-level="10.2" data-path="organizingdata.html"><a href="organizingdata.html#what-information-to-collect-about-the-recordings"><i class="fa fa-check"></i><b>10.2</b> What information to collect about the recordings</a></li>
<li class="chapter" data-level="10.3" data-path="organizingdata.html"><a href="organizingdata.html#resources-9"><i class="fa fa-check"></i><b>10.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="storingdata.html"><a href="storingdata.html"><i class="fa fa-check"></i><b>11</b> Storing your data</a><ul>
<li class="chapter" data-level="11.1" data-path="storingdata.html"><a href="storingdata.html#resources-10"><i class="fa fa-check"></i><b>11.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="whereanalyze.html"><a href="whereanalyze.html"><i class="fa fa-check"></i><b>12</b> Where should you run automated analyses</a><ul>
<li class="chapter" data-level="12.1" data-path="whereanalyze.html"><a href="whereanalyze.html#resources-11"><i class="fa fa-check"></i><b>12.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="evaluating.html"><a href="evaluating.html"><i class="fa fa-check"></i><b>13</b> Evaluating your automated analyses</a><ul>
<li class="chapter" data-level="13.1" data-path="evaluating.html"><a href="evaluating.html#basic-concepts"><i class="fa fa-check"></i><b>13.1</b> Basic concepts</a></li>
<li class="chapter" data-level="13.2" data-path="evaluating.html"><a href="evaluating.html#additional-concepts"><i class="fa fa-check"></i><b>13.2</b> Additional concepts</a></li>
<li class="chapter" data-level="13.3" data-path="evaluating.html"><a href="evaluating.html#resources-12"><i class="fa fa-check"></i><b>13.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="accuracy.html"><a href="accuracy.html"><i class="fa fa-check"></i><b>14</b> Accuracy of automated analyses</a><ul>
<li class="chapter" data-level="14.1" data-path="accuracy.html"><a href="accuracy.html#lena-software"><i class="fa fa-check"></i><b>14.1</b> LENA Software</a></li>
<li class="chapter" data-level="14.2" data-path="accuracy.html"><a href="accuracy.html#aclew-tools"><i class="fa fa-check"></i><b>14.2</b> ACLEW tools</a><ul>
<li class="chapter" data-level="14.2.1" data-path="accuracy.html"><a href="accuracy.html#voice-type-classifier"><i class="fa fa-check"></i><b>14.2.1</b> Voice type classifier</a></li>
<li class="chapter" data-level="14.2.2" data-path="accuracy.html"><a href="accuracy.html#alice"><i class="fa fa-check"></i><b>14.2.2</b> ALICE</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="accuracy.html"><a href="accuracy.html#resources-13"><i class="fa fa-check"></i><b>14.3</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="humanannotation.html"><a href="humanannotation.html"><i class="fa fa-check"></i><b>15</b> Human annotation</a><ul>
<li class="chapter" data-level="15.1" data-path="humanannotation.html"><a href="humanannotation.html#checking-accuracy"><i class="fa fa-check"></i><b>15.1</b> Checking accuracy</a><ul>
<li class="chapter" data-level="15.1.1" data-path="humanannotation.html"><a href="humanannotation.html#how-to-annotate-and-run-the-evaluation"><i class="fa fa-check"></i><b>15.1.1</b> How to annotate and run the evaluation</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="humanannotation.html"><a href="humanannotation.html#doing-things-automated-analyses-could-do-but-dont-do-quite-well-yet"><i class="fa fa-check"></i><b>15.2</b> Doing things automated analyses could do but don’t do quite well yet</a></li>
<li class="chapter" data-level="15.3" data-path="humanannotation.html"><a href="humanannotation.html#getting-complementary-information"><i class="fa fa-check"></i><b>15.3</b> Getting complementary information</a></li>
<li class="chapter" data-level="15.4" data-path="humanannotation.html"><a href="humanannotation.html#overarching-questions-deciding-which-sections-to-annotate"><i class="fa fa-check"></i><b>15.4</b> Overarching questions: Deciding which sections to annotate</a><ul>
<li class="chapter" data-level="15.4.1" data-path="humanannotation.html"><a href="humanannotation.html#how-much-audio-should-be-annotated-in-total"><i class="fa fa-check"></i><b>15.4.1</b> How much audio should be annotated (in total)?</a></li>
</ul></li>
<li class="chapter" data-level="15.5" data-path="humanannotation.html"><a href="humanannotation.html#how-should-the-clips-be-selected"><i class="fa fa-check"></i><b>15.5</b> How should the clips be selected?</a><ul>
<li class="chapter" data-level="15.5.1" data-path="humanannotation.html"><a href="humanannotation.html#unbiased-sampling"><i class="fa fa-check"></i><b>15.5.1</b> Unbiased sampling</a></li>
<li class="chapter" data-level="15.5.2" data-path="humanannotation.html"><a href="humanannotation.html#targeted-samplers"><i class="fa fa-check"></i><b>15.5.2</b> Targeted samplers</a></li>
</ul></li>
<li class="chapter" data-level="15.6" data-path="humanannotation.html"><a href="humanannotation.html#what-should-be-the-length-of-each-clip"><i class="fa fa-check"></i><b>15.6</b> What should be the length of each clip?</a></li>
<li class="chapter" data-level="15.7" data-path="humanannotation.html"><a href="humanannotation.html#softwares-for-human-annotation"><i class="fa fa-check"></i><b>15.7</b> Softwares for human annotation</a></li>
<li class="chapter" data-level="15.8" data-path="humanannotation.html"><a href="humanannotation.html#resources-14"><i class="fa fa-check"></i><b>15.8</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="secondaryanalyses.html"><a href="secondaryanalyses.html"><i class="fa fa-check"></i><b>16</b> Secondary analyses</a><ul>
<li class="chapter" data-level="16.1" data-path="secondaryanalyses.html"><a href="secondaryanalyses.html#resources-15"><i class="fa fa-check"></i><b>16.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="results.html"><a href="results.html"><i class="fa fa-check"></i><b>17</b> Writing up results</a><ul>
<li class="chapter" data-level="17.1" data-path="results.html"><a href="results.html#scientific-venue"><i class="fa fa-check"></i><b>17.1</b> Scientific venue</a></li>
<li class="chapter" data-level="17.2" data-path="results.html"><a href="results.html#families"><i class="fa fa-check"></i><b>17.2</b> Families</a></li>
<li class="chapter" data-level="17.3" data-path="results.html"><a href="results.html#communities"><i class="fa fa-check"></i><b>17.3</b> Communities</a></li>
<li class="chapter" data-level="17.4" data-path="results.html"><a href="results.html#resources-16"><i class="fa fa-check"></i><b>17.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="datadonation.html"><a href="datadonation.html"><i class="fa fa-check"></i><b>18</b> Data donation / Transfer</a><ul>
<li class="chapter" data-level="18.1" data-path="datadonation.html"><a href="datadonation.html#you-dont-have-data-yet"><i class="fa fa-check"></i><b>18.1</b> You don’t have data yet</a></li>
<li class="chapter" data-level="18.2" data-path="datadonation.html"><a href="datadonation.html#you-already-have-data"><i class="fa fa-check"></i><b>18.2</b> You already have data</a><ul>
<li class="chapter" data-level="18.2.1" data-path="datadonation.html"><a href="datadonation.html#you-have-already-shared-your-data-in-a-scientific-repository"><i class="fa fa-check"></i><b>18.2.1</b> You have already shared your data in a scientific repository</a></li>
<li class="chapter" data-level="18.2.2" data-path="datadonation.html"><a href="datadonation.html#you-havent-shared-your-data-in-a-scientific-repository-yet-but-you-are-willing-to-do-so"><i class="fa fa-check"></i><b>18.2.2</b> You haven’t shared your data in a scientific repository yet but you are willing to do so</a></li>
<li class="chapter" data-level="18.2.3" data-path="datadonation.html"><a href="datadonation.html#you-dont-want-to-upload-your-data-on-a-scientific-repository-but-you-are-willing-to-create-a-license"><i class="fa fa-check"></i><b>18.2.3</b> You don’t want to upload your data on a scientific repository but you are willing to create a license</a></li>
<li class="chapter" data-level="18.2.4" data-path="datadonation.html"><a href="datadonation.html#none-of-the-above-mentioned-options-works-well-for-you"><i class="fa fa-check"></i><b>18.2.4</b> None of the above mentioned options works well for you</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="field.html"><a href="field.html"><i class="fa fa-check"></i><b>19</b> Tips for field workers</a><ul>
<li class="chapter" data-level="19.1" data-path="field.html"><a href="field.html#devices"><i class="fa fa-check"></i><b>19.1</b> Devices</a></li>
<li class="chapter" data-level="19.2" data-path="field.html"><a href="field.html#clothing-1"><i class="fa fa-check"></i><b>19.2</b> Clothing</a></li>
<li class="chapter" data-level="19.3" data-path="field.html"><a href="field.html#logistics-1"><i class="fa fa-check"></i><b>19.3</b> Logistics</a></li>
<li class="chapter" data-level="19.4" data-path="field.html"><a href="field.html#storage"><i class="fa fa-check"></i><b>19.4</b> Storage</a></li>
<li class="chapter" data-level="19.5" data-path="field.html"><a href="field.html#resources-17"><i class="fa fa-check"></i><b>19.5</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="multilingualsettings.html"><a href="multilingualsettings.html"><i class="fa fa-check"></i><b>20</b> Performance across languages &amp; for bilingual or multilingual settings</a><ul>
<li class="chapter" data-level="20.1" data-path="multilingualsettings.html"><a href="multilingualsettings.html#resources-18"><i class="fa fa-check"></i><b>20.1</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="alternatives.html"><a href="alternatives.html"><i class="fa fa-check"></i><b>21</b> Alternative / Additional tests</a><ul>
<li class="chapter" data-level="21.1" data-path="alternatives.html"><a href="alternatives.html#if-you-want-alternative-measures-of-interaction"><i class="fa fa-check"></i><b>21.1</b> If you want alternative measures of interaction</a></li>
<li class="chapter" data-level="21.2" data-path="alternatives.html"><a href="alternatives.html#if-you-want-use-a-more-established-instrument"><i class="fa fa-check"></i><b>21.2</b> If you want use a more established instrument</a><ul>
<li class="chapter" data-level="21.2.1" data-path="alternatives.html"><a href="alternatives.html#children-0-3-years"><i class="fa fa-check"></i><b>21.2.1</b> Children 0-3 years</a></li>
<li class="chapter" data-level="21.2.2" data-path="alternatives.html"><a href="alternatives.html#children-aged-3-5-years"><i class="fa fa-check"></i><b>21.2.2</b> Children aged 3-5 years</a></li>
<li class="chapter" data-level="21.2.3" data-path="alternatives.html"><a href="alternatives.html#additional-tests"><i class="fa fa-check"></i><b>21.2.3</b> Additional tests</a></li>
</ul></li>
<li class="chapter" data-level="21.3" data-path="alternatives.html"><a href="alternatives.html#references"><i class="fa fa-check"></i><b>21.3</b> References</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Long-form recordings: From A to Z</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="humanannotation" class="section level1">
<h1><span class="header-section-number">Chapter 15</span> Human annotation</h1>
<p>Automated analyses are great because they can capture patterns over all of the data for each child, but you may consider also doing some annotations by humans. We see three purposes for human annotation:</p>
<ol style="list-style-type: decimal">
<li>To check accuracy of automated analyses</li>
<li>To do things that automated analyses don’t yet do</li>
<li>To complement with more qualitative data</li>
</ol>
<p>We provide some pointers for each of the three goals.</p>
<div id="checking-accuracy" class="section level2">
<h2><span class="header-section-number">15.1</span> Checking accuracy</h2>
<p>If you are working with a population for which automated tools’ accuracy has not been checked against a human standard, it is a good idea to do some annotation to this end. We provide you with information about how to do this here. Note that if you are working with English-speaking or bilingual children in North America, there have been several validation studies, so it is probably not a good use of your resources to do yet another validation study.</p>
<p>Before you start, we recommend a literature search starting from Cristia, Bulgarelli, &amp; Bergelson (2020)’s systematic review (in this video’s References), to make sure that there really is no reliability data for a similar sample. If no reliability studies exist, then you can use the data you have collected in a pilot or your study. Do not collect data in a different environment from the one you are aiming to use the recorder in. In particular, do not record yourself reading a book next to the recording device in a sound-proof booth. The accuracy in such a setting will tell you very little about the accuracy of the algorithms in your spontaneous, naturalistic, child-centered sample.</p>
<div id="how-to-annotate-and-run-the-evaluation" class="section level3">
<h3><span class="header-section-number">15.1.1</span> How to annotate and run the evaluation</h3>
<p>Imagine that you decided to draw 10 clips x 2 minutes randomly from each of 10 children. This is about 3h 20min of data, which takes roughly 90h to annotate, in our experience. We recommend training annotators using the <a href="https://osf.io/b2jep/">ACLEW Annotation Scheme</a>, which has an online test annotators can go through to ensure accuracy of their own annotations.</p>
<p>Once the manual annotations are complete, the machine annotations can be extracted and compared against the human annotations easily, provided you are using ChildProject to organize your data. We have a separate video where we introduce you to ChildProject <a href="childproject.html#childproject">6</a>.</p>
<p>In a nutshell, this will allow you to extract key classification accuracy measures used here (false alarm rate, miss rate, confusion rate and the derived identification error rate), as well as CVC, CTC, and AWC comparing LENA® and human annotations. We explain all of these terms in the video about Reliability and Validity.</p>
<p>We insist that re-using our code is only possible “off the shelf” for manual annotations made using the ACLEW Annotation Scheme for the annotations, and ChildProject for organizing your data, although if you know how to program you can also adapt it to other schemata.</p>
</div>
</div>
<div id="doing-things-automated-analyses-could-do-but-dont-do-quite-well-yet" class="section level2">
<h2><span class="header-section-number">15.2</span> Doing things automated analyses could do but don’t do quite well yet</h2>
<p>You may be interested in quantifying speech addressed to the key child versus to others, to separate child-directed from overheard speech; or perhaps you want to estimate the complexity of the child’s vocalizations in terms of the sounds the child produces. Both of these goals are almost within reach of automated analyses. It is likely that both can be done with fairly local information – that is, by listening to a sentence, you may be able to tell who it is spoken to. You may not need to know the whole context of the conversation.</p>
<p>If that describes the kind of information you are hoping to extract, we strongly recommend considering to rely on citizen scientists – see the Video on <a href="irb.html#irb">8</a> for information on seeking IRB approval for this, and Zooniverse for an excellent citizen science platform. There are ways of processing your data so that it can be hosted in such a platform without revealing participants’ identity or personal information.</p>
<p>In some cases, you do need a little context – at the very least to decide whether a child vocalization is meaningful or not. At this point, this is not a task that can be solved by a machine, and in fact it takes quite some training for humans to do it reliably and replicably. If this is the kind of thing you were thinking of, we recommend looking at Mendoza and Fausey’s and Soderstrom et al’s papers in the Resources section for more ideas.</p>
</div>
<div id="getting-complementary-information" class="section level2">
<h2><span class="header-section-number">15.3</span> Getting complementary information</h2>
<p>You may want to get ideas of the warmth of the interactions, or the contexts in which different languages are used in a multilingual household. In this case, we recommend the work of Cychosz, Villanueva, &amp; Weisleder for more ideas.</p>
</div>
<div id="overarching-questions-deciding-which-sections-to-annotate" class="section level2">
<h2><span class="header-section-number">15.4</span> Overarching questions: Deciding which sections to annotate</h2>
<p>Human annotations require to define a sampling process, i.e. an algorithm to decide which portions of audio should be annotated. This raises a few questions, among which:</p>
<ul>
<li>How much audio should be annotated (in total)?</li>
<li>How should the clips be selected?</li>
<li>What should be the length of each clip?</li>
</ul>
<p>Of course, the answer to each of this question depends on your research goals, but we provide here some considerations to help you decide.</p>
<div id="how-much-audio-should-be-annotated-in-total" class="section level3">
<h3><span class="header-section-number">15.4.1</span> How much audio should be annotated (in total)?</h3>
<p>Most often, the total amount of audio that can be annotated is heavily constrained by the availability of human resources, and can be considered fixed. For instance, you have a budget for a research assistant, or limited time because you need to turn in your thesis. Even in these cases, consider our arguments here, because they will let you think about the kind of phenomenon you can look at.</p>
<p>The quality of estimations based off partial annotations primarily depends on the volume of audio that have been annotated. There are several reasons to use partial annotations to extrapolate metrics over the whole data:</p>
<ul>
<li>No automated tools exist or are accurate enough for the task that is being considered</li>
<li>Automated tools exist, but due to limited resources and/or time, they cannot be run over whole recordings.</li>
</ul>
<p>To illustrate the effect of subsampling on the quality of estimates derived from annotations, we used annotations generated by the Voice Type Classifier over recordings of the Bergelson dataset <span class="citation">(Bergelson <a href="#ref-bergelson" role="doc-biblioref">2015</a>)</span>. We compared the estimates based from subsamples with various annotation budgets to the values obtained for the whole recordings.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:budget-impact"></span>
<img src="figures/budget_impact.png" alt="Relative error between speech duration estimates based on subsamples and the values for the whole recordings, using the Voice Type Classifier. (periodic sampling; 1-minute clips.)"  />
<p class="caption">
Figure 15.1: Relative error between speech duration estimates based on subsamples and the values for the whole recordings, using the Voice Type Classifier. (periodic sampling; 1-minute clips.)
</p>
</div>
<p>The level of accuracy to achieve, and therefore the volume of audio to annotate, depend on the size of the effects being studied.</p>
<!-- this also demonstrates how the conclusions to be drawn from manual annotations may vary according to how they were selected. -->
</div>
</div>
<div id="how-should-the-clips-be-selected" class="section level2">
<h2><span class="header-section-number">15.5</span> How should the clips be selected?</h2>
<div id="unbiased-sampling" class="section level3">
<h3><span class="header-section-number">15.5.1</span> Unbiased sampling</h3>
<p>For many tasks, the clips should be selected in a way that does not induce biases in the results. For instance, if one plans to estimate the quantity of speech produced or heard by the child, then it is important to use unbiased estimators. In order to evaluate the performance of automated tools, it might also be preferable to use unbiased annotations which can assess the behavior of the algorithms in a variety of contexts rather than for a subset of configurations.</p>
<p>We can cite at least two ways of achieving such an unbiased sampling:</p>
<ul>
<li>By choosing each clip at random, independently from past choices</li>
<li>By sampling the audio periodically (so that the position of the first clip completely determines the positions of subsequent clips)</li>
</ul>
<p><strong>Q: shouldn’t be better to sample regions where we know the child to be talking a lot, or where there are lots of conversational exchanges between adults and the key child? </strong></p>
<p>A similar question is whether you cannot use annotations that you have sampled like that, for qualitative analyses, to have an idea of accuracy. If you have a limited annotation budget, of course re-using your annotations this way is better than nothing at all, but do note that if you sample your recording non-randomly, it is less certain that your conclusions based on such clips will generalize to the whole of the recording.</p>
<p>For instance, imagine that you were trying to decide whether the child talks little or a lot based on sections that have been selected because the child talks – that would make it difficult to know because you’ve purposefully selected regions where they are talking, giving you a sort of ceiling effect.</p>
<p><strong>Q: When I sample randomly or periodically, I don’t have a lot of category X - is that a problem?</strong></p>
<p>If there are no samples of a given category, then accuracy of that category cannot be evaluated; and if there are only a few, then it is possible that these are special in some way and accuracy estimates may not generalize well to others. So if the sections you end up with have no “other child” or “male” speech, then perhaps you’ll be uncertain of how well the algorithm picks up these voices.</p>
</div>
<div id="targeted-samplers" class="section level3">
<h3><span class="header-section-number">15.5.2</span> Targeted samplers</h3>
<p>Although unbiased samplers are necessary for many purposes, targeted samplers may be useful in a number of cases.
One such case is the training of a computational model that requires a great amount of manual annotations.
Usually, annotations obtained from portions of audio that contain little to no speech are uninformative for these models.
Therefore, it may be much more efficient to let humans annotate portions of the recordings that have a higher chance of containing speech.</p>
<p>For this purpose, one could at least conceive two strategies:</p>
<ul>
<li>Using a Voice Activation Detection model or more sophisticated models (e.g. linguistic unit estimation, speaker diarization, etc.)</li>
<li>Choosing portions of the audio that have a strong signal energy</li>
</ul>
<p>Using sophisticated models may yield a more efficient pre-selection. However, it is harder to predict the effects of their own biases, and a cruder
selection method (such as the energy detection) may still show good results while being less arbitrary.
Below, using 2 hours and 40 minutes of human annotations from the Solomon Islands corpus, we show how selecting portions of audio
based on the signal energy alone can increase the amount of speech.
This benchmark is based on the <a href="https://childproject.readthedocs.io/en/latest/samplers.html#energy-based-sampler">Energy Detection Sampler of the ChildProject package</a>.
We used a 50 Hz - 3000 Hz passband in order to increase the signal/noise ratio.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:energy-sampler-n"></span>
<img src="figures/energy_sampler_n.png" alt="Fraction of 30 second windows above some energy quantile that contain speech from a given speaker class. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult."  />
<p class="caption">
Figure 15.2: Fraction of 30 second windows above some energy quantile that contain speech from a given speaker class. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:energy-sampler-time"></span>
<img src="figures/energy_sampler_time.png" alt="Average speech time for each speaker class depending on the energy quantile. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult."  />
<p class="caption">
Figure 15.3: Average speech time for each speaker class depending on the energy quantile. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult.
</p>
</div>
<p>This example shows that the energy-based sampler clearly suppresses silence.
However, it also induces a bias by favoring some classes (CHI and FEM) over others (MAL and OCH).
One explanation is that the first two (key child and female adult) are closer to the device (especially the key child, by design)
and thus sound louder on average.</p>
</div>
</div>
<div id="what-should-be-the-length-of-each-clip" class="section level2">
<h2><span class="header-section-number">15.6</span> What should be the length of each clip?</h2>
<p>The answer, again, will depend on the research question. For instance, exploring conversational behaviors may require longer clips than only assessing the amount of speech. However, here we show a way in which the choice of the sampling strategy may have a significant impact.</p>
<p>We used automated annotations derived from 500 recordings from the Bergelson dataset <span class="citation">(Bergelson <a href="#ref-bergelson" role="doc-biblioref">2015</a>)</span>. Automated annotations are easy to produce in large amounts; this allows us to simulate a sampling method using semi-realistic distributions of speech.</p>
<p>For a constant annotation budget of 20 minutes per recording, we estimated the total amount of speech for each recording using only annotations extracted with two sampling strategies - random clips vs periodic (i.e. evenly spaced) clips - and different lengths of clips.
This allowed us to estimate the relative difference between the quantities of speech estimated from 20 minutes of annotations and the true quantities.</p>
<p>The results are shown below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sampling-strategies"></span>
<img src="figures/speech_estimation_strategies.png" alt="Performance of several sampling strategies. (500 recordings; constant annotation budget of 20 minutes per recording.)"  />
<p class="caption">
Figure 15.4: Performance of several sampling strategies. (500 recordings; constant annotation budget of 20 minutes per recording.)
</p>
</div>
<p>It can be seen that the estimations are much more accurate when using many short clips rather than a few longer clips. The periodic sampling also yields more accurate results than random sampling.
This result is rather intuitive: annotating a portion of audio in close temporal proximity with a previous portion will yield much less information, because the two portions are correlated.
Using annotations that are farther from each other helps probe a wider variety of configurations and thus yields better speech quantity estimates.</p>
<p>This does not mean, of course, that one should annotate extremely short clips. First, in practice, two 1-minute clip will take longer to annotate than one 2-minute clips, especially if annotators needs to listen to the context to make better decisions. Reducing the length of the clips may thus rapidly decrease the efficiency of the annotation process. Moreover, some estimators, such as vocalization counts, may be strongly biased by using short clips, due to the increased significance of boundary effects.</p>
<p>Also, there is one exception to this general recommendation of prioritizing shorter over longer clips: Often, colleagues use Pearson correlation coefficients between the metrics derived from the algorithm (e.g. vocalization counts or speech duration) and the human coding of the same sections to look at reliability. We talked about the fact that there are many ways to quantify the quality of automated annotations from a gold standard established by expert annotators in <a href="evaluating.html#evaluating">13</a>. We looked at whether this measure was affected by the choice of the clip duration using the Bergelson dataset <span class="citation">(Bergelson <a href="#ref-bergelson" role="doc-biblioref">2015</a>)</span>. As above, we used LENA and VTC as our two algorithms (having human annotation would have been great, but we don’t have that for the whole 500 recordings). We then extracted clips of e.g. 1 minute of duration from both the VTC and LENA annotations and calculated the correlations across e.g. the child vocalization counts by the two algorithms. We repeated for other speech metrics (both vocalization counts and speech duration for both children and adults); and we did the same for clips of other lengths. Results are shown below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pearson-length"></span>
<img src="figures/pearson_clip_length.png" alt="Pearson correlation between LENA and VTC derived metrics as a function of clips length. (500 recordings; periodic sampling; constant annotation budget of 60 minutes per recording.)"  />
<p class="caption">
Figure 15.5: Pearson correlation between LENA and VTC derived metrics as a function of clips length. (500 recordings; periodic sampling; constant annotation budget of 60 minutes per recording.)
</p>
</div>
<p>We can observe a clear increase in the correlation coefficients as the length of the clips increases. This means Pearson correlation coefficients for these metrics are not robust to the length of the clips used to evaluate the algorithms, and actually benefit from longer clips. But we know longer clips have issues: they can have greater estimation errors (as we saw above).</p>
<p>In contrast, Recall and Precision remain stable for varying clip lengths.</p>
<p>Our conclusion is that perhaps the field should turn to these other metrics, but in the meantime, you should be really careful when thinking about why you are doing annotation, what you are going to use them for, and how they may be suboptimal for other goals.</p>
</div>
<div id="softwares-for-human-annotation" class="section level2">
<h2><span class="header-section-number">15.7</span> Softwares for human annotation</h2>

<table class="table" style="font-size: 10px; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Software
</th>
<th style="text-align:left;">
Key strenght
</th>
<th style="text-align:left;">
System
</th>
<th style="text-align:left;">
Input
</th>
<th style="text-align:left;">
Multitier
</th>
<th style="text-align:left;">
Closed vocab
</th>
<th style="text-align:left;">
Free text
</th>
<th style="text-align:left;">
Spectogram
</th>
<th style="text-align:left;">
Large files
</th>
<th style="text-align:left;">
Interoperability
</th>
<th style="text-align:left;">
Modes
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<a href="https://www.fon.hum.uva.nl/praat/">Praat</a> (Boersm a, 2009)
</td>
<td style="text-align:left;">
ideal for acoustic phonetics
</td>
<td style="text-align:left;">
support
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
timed
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
limited
</td>
<td style="text-align:left;">
CLAN, Phon
</td>
<td style="text-align:left;">
both
</td>
</tr>
<tr>
<td style="text-align:left;">
[Phon] (<a href="https://www.phon.ca/phon-manual/getting_started.html" class="uri">https://www.phon.ca/phon-manual/getting_started.html</a>) (Rose et al. 2007)
</td>
<td style="text-align:left;">
ideal for phonological level
</td>
<td style="text-align:left;">
support, OS
</td>
<td style="text-align:left;">
AV
</td>
<td style="text-align:left;">
both
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
Praat, CLAN
</td>
<td style="text-align:left;">
both
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="http://transag.sourceforge.net/">Transcri berAG</a>
</td>
<td style="text-align:left;">
‘recommended’ by LENA
</td>
<td style="text-align:left;">
OS
</td>
<td style="text-align:left;">
A
</td>
<td style="text-align:left;">
timed
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
none
</td>
<td style="text-align:left;">
easy
</td>
</tr>
<tr>
<td style="text-align:left;">
[Datavyu] (<a href="http://www.datavyu.org/" class="uri">http://www.datavyu.org/</a>) (Datavyu Team, 2014)
</td>
<td style="text-align:left;">
User-defined key strokes
</td>
<td style="text-align:left;">
support, OS
</td>
<td style="text-align:left;">
AV+
</td>
<td style="text-align:left;">
untimed
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
none
</td>
<td style="text-align:left;">
both
</td>
</tr>
<tr>
<td style="text-align:left;">
[ELAN] (<a href="https://tla.mpi.nl/tools/tla-tools/elan/" class="uri">https://tla.mpi.nl/tools/tla-tools/elan/</a>) (Sloetjes &amp; Wittenbu rg, 2008)
</td>
<td style="text-align:left;">
Multi-stream, use of template, interopera ble
</td>
<td style="text-align:left;">
support, OS
</td>
<td style="text-align:left;">
AV
</td>
<td style="text-align:left;">
both
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
CLAN, Praat Transcriber, AG, …
</td>
<td style="text-align:left;">
both
</td>
</tr>
<tr>
<td style="text-align:left;">
[CLAN] (<a href="http://childes.psy.cmu.edu/" class="uri">http://childes.psy.cmu.edu/</a>) (MacWhi nney, 2000)
</td>
<td style="text-align:left;">
Ideal for lexicon and grammar
</td>
<td style="text-align:left;">
support
</td>
<td style="text-align:left;">
AV
</td>
<td style="text-align:left;">
untimed
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
no
</td>
<td style="text-align:left;">
yes
</td>
<td style="text-align:left;">
Praat, Phon, ELAN
</td>
<td style="text-align:left;">
both
</td>
</tr>
</tbody>
</table>

</div>
<div id="resources-14" class="section level2">
<h2><span class="header-section-number">15.8</span> Resources</h2>
<p>Cristia, A., Lavechin, M., Scaff, C., Soderstrom, M., Rowland, C., Räsänen, O., Bunce, J., &amp; Bergelson, E. (2020) A thorough evaluation of the Language Environment Analysis (LENA) system. Behavior Research Methods. <a href="https://osf.io/mxr8s">preprint</a> <a href="https://osf.io/zdg6s/">online resource</a></p>
<p>Cristia, A., Bulgarelli, F., &amp; Bergelson, E. (2020). Accuracy of the Language Environment Analysis (LENATM) system segmentation and metrics: A systematic review. Journal of Speech, Language, and Hearing Research. <a href="https://osf.io/4nhms/">online resources, including pdf</a></p>
<p>Cychosz, M., Villanueva, A., &amp; Weisleder, A. (2020). Efficient estimation of children’s language exposure in two bilingual communities. <a href="https://psyarxiv.com/dy6v2/download?format=pdf">pdf</a></p>
<p>Mendoza, J. K., &amp; Fausey, C. M. (2021). Quantifying everyday ecologies: Principles for manual annotation of many hours of infants’ lives. <a href="https://psyarxiv.com/79vwe/download?format=pdf">pdf</a></p>
<p>Soderstrom, M., Casillas, M., Bergelson, E., Rosemberg, C., Alam, F., Warlaumont, A. S., &amp; Bunce, J. (2021). Developing A Cross-Cultural Annotation System and MetaCorpus for Studying Infants’ Real World Language Experience. Collabra: Psychology, 7(1), 23445. <a href="https://psyarxiv.com/bf63y/download?format=pdf">pdf</a></p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-bergelson">
<p>Bergelson, Elika. 2015. “HomeBank Bergelson Corpus.” TalkBank. <a href="https://doi.org/10.21415/T5PK6D">https://doi.org/10.21415/T5PK6D</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="accuracy.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="secondaryanalyses.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["test book.pdf", "test book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
