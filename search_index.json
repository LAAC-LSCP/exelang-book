[["index.html", "Long-form recordings: From A to Z Preface", " Long-form recordings: From A to Z Sara Pisani Lucas Gautheron Alejandrina Cristia Ecole Normale Supérieure, PSL Research University, Departement d’etudes cognitives, Laboratoire de Sciences Cognitives et Psycholinguistique (ENS, EHESS, CNRS) alejandrina.cristia@ens.fr 2021-10-06 Preface This bookdown contains the scripts of instructional videos created in the context of the ExELang Project (exelang.fr). Alejandrina Cristia is grateful for the funding and institutional support from Agence Nationale de la Recherche (ANR-17-CE28-0007 LangAge, ANR-16-DATA-0004 ACLEW, ANR-14-CE30-0003 MechELex, ANR-17-EURE-0017); a J. S. McDonnell Foundation Understanding Human Cognition Scholar Award; and an ERC Consolidator grant (ExELang, 2021-2026). The funders had no role in the production of this bookdown or the associated videos. Alejandrina Cristia assumes full responsibility for the contents. Nonetheless, we are grateful to people who contributed to the contents more or less directly: Camila Scaff, on whose EMLAR class some of the visuals and content organization are based Emmanuel Dupoux, Xuan Nga Cao, and Manel Khentout for their expertise on the babyloggers Marvin Lavechin, for contributions on the explanation of the automated analysis software Lucas Gautheron, on the ChildProject tutorial sections Clementine Fourrier, for contacts with the ENS Audiovisual department Catherine Urban, for administrative support and particularly, to our collaborators in related projects, from whom we have learned tremendously: Elika Bergelson and Marisa Casillas, for frequent discussions ranging from data collection to analyses and beyond Marisa Casillas, Jonathan Stieglitz, Gandhi Yetish, and Camila Scaff, for insights regarding the use of long-form recordings in field conditions, and mentoring before, during, and after fieldwork Florencia Lopez Boo and Maria de la Paz Ferro, for irreplaceable discussions about the use of long-form recordings in the context of large randomized control trials evaluating at scale social programs Fiona Burns, Pauline Grosjean, and Fiona Burns, for invaluable discussions about application to large randomized control trials in field conditions "],["introduction.html", "Introduction 0.1 Resources", " Introduction Hello, I’m Alex Cristia, principal investigator of the ExELang Project. For this Project, we are creating a series of videos to help researchers everywhere use long-form recordings to collect data on early language development. Longform recordings are recordings made over an extended period of time, for instance, one whole day or even several days. They are often made using a wearable device, for instance, a recorder that is clipped on the child’s clothing. Since each recording lasts several hours (often 10 or more) you will have many hours of audio for each child, and often hundreds or thousands across all children in your sample. So you cannot analyze this audio manually, by listening to it and transcribing what you hear, but instead you will need to use some kind of automated system, at least as a first pass. Longform recordings are most appropriate if you want to look for broad phenomena that happen frequently, for example how frequently the child vocalizes or cries; you can get estimates and reasons why. They are not ideal if you want to look for narrower phenomena that are rare or don’t have very clear acoustic cues. Imagine you are interested in knowing whether the child makes systematic errors when they talk, for example if they make systematic errors in the past simple form. This is difficult to study because at present we don’t have an automated system that transcribes what the child precisely says (e.g. walk vs walked). What will this series cover then? If you are interested in using longform recordings, we are going to help you with everything you need to know, from how you decide to collect data, to hardware and software, clothing, how to ask permission to your IRB, human annotation, piloting, how to share data. We have tried to distill nearly 10 years of using the technique, and literally hundreds of hours collecting, analyzing, discussing the technique, and teaching it to others. Our hope is that this series will serve as a reference: You can quickly get an idea of what are the things you need to think about and do, and you can go back to the videos of different steps as you progress in your project. Each video will end with a series of references and links where you can get more in depth information. We are also making the script of this video available as a book - the link is at the end too. You can ask questions and make comments as issues to the book or in the discussion section of each video. We particularly want to draw your attention to a flowchart we created with Marisa Casillas, which can help you decide Figure 0.1: Flowchart of key decisions for those considering a study with LFSE recordings. Those researchers whose path ends with an “X” should instead consider non-LFSE approaches. We hope this is useful to you! 0.1 Resources This book Casillas, M. &amp; Cristia, A. (2019). A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings, Collabra. link "],["hardware.html", "Chapter 1 Hardware 1.1 LENA hardware 1.2 Non-LENA hardware 1.3 The bottom line regarding hardware 1.4 Summary 1.5 Resources", " Chapter 1 Hardware In this video we will be talking about hardware. We distinguish hardware from software. The hardware is the physical equipment you will use to collect the audio data, while the software is the program that you’ll run later to extract and analyze data. We will be talking about software in a separate dedicated video. Although we separate hardware and software because they are different, in the context of long-form recordings, hardware and software cannot be chosen independently from each other because the most commonly used system actually entails both a hardware and a software portion. This is the LENA system. The LENA Foundation created hardware (a recording device they call DLP, digital language processor) as well as a software system, and they can only be used together. So if you are deciding whether to use LENA or not, then you either go all in or all out. This means that if you use the LENA software you need to use the LENA hardware and vice versa. When we compare hardware, we will take into account 6 features: ease of use cost length of an audio recording bout total recording storage capacity childproof any special features of the device Let me explain each. In ease of use, we consider how light the device is, how easy it is to make it into a wearable, and the simplicity of the user interface. The user interface is an important consideration for those allowing their participants to control when the recorder is on and off, which is important for ethical purposes, as we explain in the Video on IRB – see Chapter 8. For the cost, we’ll typically talk about how much each individual recorder costs. As for length of the recording bout and total storage, these are actually not necessarily identical. That is, typically, the length of a continuous or intermittent recording is determined by its battery, and not the data storage or memory limitation. So when we talk about the length of a recording bout, we mean with a single battery charge. In contrast, total storage is how much data you can have in the recording device before you need to extract the data. So if you are working with families that can recharge the device, you can also ask them to record several days (up to a week, depending on the device). This is determined by the device’s memory limitations. Also, we talk about devices that are more or less childproof – and by this we mean that it’ll be relatively hard for the child to break it by e.g. gaining access to it and stepping on it, or dropping water onto it. All the devices we talk about are child-friendly, in the sense that they are lightweight. For each recording device type, we’ll discuss also any specific or special features, such as their recording quality. 1.1 LENA hardware Let’s start with the most common device used, which is the LENA Foundation’s DLP. The LENA Foundation’s hardware (like its software) is durable, stable, and simple to use by all participants involved in developmental language studies (researcher, practitioner, parent, and child). At present it costs about 400 US$ a piece and you get a discount if you buy several of them. It allows you to record 24h straight and then the battery runs out. You can also ask the family who is doing the recording to recharge it, which will allow them to record 3 times. When the recording goes up to 72h, the memory is full, and then you need to extract the data before you can use it again. It is perfectly childproof. Regarding special features, please note that you need to pay for the LENA software separately - they have several licenses, and you can ask for a quote, but just to give you an idea, you probably need about 5000 US$ to get started. One unique feature of LENA is that they have an accurate internal clock, that allows you to more easily know when recording was started and stopped. 1.2 Non-LENA hardware All other hardware options require you to use a different software – you cannot use the LENA software unless you use their hardware too. So if you choose a non-LENA hardware, you’ll need to make a separate set of decisions for the software. We’ll be discussing non-LENA software options in detail in a dedicated video – see Chapter 5. In reality, you could use any alternative recording devices as long as it is wearable and lightweight. In fact, even a device that wasn’t designed to be worn can be adapted for this purpose, for instance by using a money or paper clip to attach it to someone’s clothing. (We discuss clothing in a dedicated video – see Chapter 3.) 1.2.1 iPods Some researchers have adopted iPods, which are as light as the LENA device. One advantage of iPods is that you can use a platform to program them to collect data intermittently, which is ideal if you want to record snippets here and there rather than continuously. They have several disadvantages. To begin with, they are quite pricey, they are not childproof, and the interface on the actual hardware is not as simple to use as other alternatives we discuss here. Therefore, it may not be the best adapted particularly if the families are going to turn recordings on and off by themselves. They have more seldom been used among children, and we have no first-hand experience with them in this setting, so we cannot tell you for sure what the length of a recording bout is, nor their total capacity. It is possible that iPods could be programmed to upload recordings to the cloud as they go, but we suspect this will limit recording bouts as this will use a good deal of battery. Recordings are time-stamped, so you can tell when each recording bout started and stopped. 1.2.2 Hand-held recorders You can also use devices that are frequently used by field linguists and psychologists, such as Olympus hand-held recorders. Note that we don’t mean you are actually going to hold them – we just mean that they are small and sturdy, so they can be adapted as wearables. This is done by using a money or paper clip, or sewing pockets onto t-shirts – we’ll talk about these options in the Clothing video – see Chapter 3. Some hand-held devices are a quite good option, because they have better audio quality than LENA, they can have higher sampling frequency and a flat frequency response rate, resulting in the highest quality among the devices we are discussing here. So if you want to look at details of the sounds people say, this may be the best choice. There are a few disadvantages of hand-held recorders. To begin with, the interface is less easy to use by researchers and parents than other options, with menus that need to be navigated. Also, they are not child-proof, so the child may be able to break them or affect the recording (e.g., by stopping it). They can be relatively heavy – although please note that some of the latest models, that do not use batteries, are as light as the LENA. Also, suppose the equipment is stolen after some recording has already happened: personal data will be lost and could fall in the hands of someone who doesn’t use it for the purpose it was supposed to, this is not under your control. Also, since Olympus recorders typically have a USB port, anyone could simply copy the data you are recording and use it for their own purposes. These options cost between 200/300 $ a piece. The price varies as several models would fit this. The one we have used had a maximum recording bout of 22 continuous hours. After this point, the recording stopped by itself. There was still some space left, so it is possible that recharging the batteries in the device would have allowed families to record several such bouts. To our knowledge, these recordings are not accurately time-stamped. 1.2.3 USB “spy” recording devices An option that our team finds very promising is a USB device that has been mass-produced, and can be found in regular outlets (e.g., on Amazon) if you look for “spy audio recording USB”. The ones we used originally are no longer being sold, but you can find a link to a similar one in the resources section of this video. USB devices are much cheaper (10-20US$ a piece) and you can find many different brands. We recommend you find a brand that time-stamps recordings, since families sometimes start and stop the recording (to make use of their choice to withhold data, or because you have asked them not to record when the child is away or asleep). The USB devices we used have a battery life of 15 hours, and most of the other people we know to have used them report a similar length of recording bouts. One disadvantage of these devices is that sometimes they fail, resulting in recordings that are only 4 hours long, or shorter. We have dealt with this by using two devices, launched at the same time – this way, if one of them fails, you always have a back-up. So each recording bout will be maximally 15 hours in length, but the devices can be recharged by plugging them to a USB charger. This will allow the family to collect several such recordings – for instance for 4 days. The precise storage capacity depends on the brand that you use. They are very easy to use by researchers and parents because they only have an ON/OFF switch. Given their small size, and the fact that they come with small cap, if you choose this option, we strongly encourage you to consider choking hazards. You can reduce this risk by removing the cap and clipping the USB to the child’s clothing in such a way that it becomes impossible for the child to pull it out. We’ll talk about how you do this in the clothing video – see Chapter 3. The technical audio quality is comparable to those of the other devices, and although to the ear the sound is less sharp than others’, automated analyses do not show a great deal of differences with other devices. As with other devices, we discussed, there is a security problem: It is very obvious in this case for users that they may be able to plug in the device onto a computer that has a USB port, and copy over the recording data, which constitutes a security hazard. 1.2.4 Babylogger Our team is currently turning to the BABYLOGGER. The Babylogger is being developed in France, and at present it is still in the piloting stage. We have bought quite a few because we think it is very promising. In our view, it has a few features that makes it more interesting than the LENA device: in fact, it has been molded on the LENA device with the goal of improving it. While it has about the same size, weight, sturdiness, and ease of use for the families as the LENA, the Babylogger has 4 microphones instead of just 1, and it contains an accelerometer, which is interesting if you want to analyze movement data. But in our opinion, the very best features of the Babylogger are the length of the recording bout and of the total storage, and the fact that recordings are encrypted. Let me explain this. The Babylogger allows more extensive recordings. As you recall, the LENA device allows you to record 24h straight, up to 72h if you recharge it. The Babylogger also allows for 24h continuous recordings. But when you recharge it, it will hold a week of recordings. Well, if I’m going to be perfectly accurate, since the babylogger has an external SD card for its storage, you can also buy a card with greater capacity, that allows families to record for even longer. Regarding encryption, recall that a problem with all other recording devices is one of privacy: external people can copy data and use it as they wish. This is not the case for the Babylogger: as far as we know, this is the only device that has on-device encryption of the data, so if the device is lost, people cannot copy data. When you buy it, you are the only one having the key to unencrypt data for the device. One downside of the babylogger is that, since it is still in the piloting stage, it’s produced in small amounts, so they come at a very expensive cost, which we estimate at 600€ each (although this depends on how many you buy and how purchases are grouped). Also, we are helping test it, and sometimes we find issues that we report to the team - so it is a device that will change in the future. 1.2.5 Wireless systems Other researchers have come up with solutions that work well only in the home, such as recording systems coupled with wireless microphones worn by the key child and selected family members. This is actually based on technology that has existed for a very long time – we think Wells 1977’s data, available from CHILDES, gives a good example of the interest of this technique. Fewer people are using this one, so we won’t talk about this option very much in this series. We have not used these first hand, so we cannot tell you the cost and ease of use for the family. We suspect that if the device controlling the recording can be plugged in, then the length of recording bouts can be quite long. Also, most portable microphones we know about should be quite easy to learn how to use, with a button to turn them on or off. These microphones tend to be way cheaper; nevertheless, they are not necessarily childproof, nor easy to use. Also, you need to set up a bit of equipment in the child’s home. Such a system is a great solution to a problem that is prevalent across all the others, which are continously worn by the child: When the child leaves the home, they will not be recorded, which poses less privacy problems and issues of incidental recording of people whose informed consent has not been established. We discuss these in the IRB video – see Chapter 8. Since you can fit several family members with microphones, this will probably result in better quality of audio for each person, and also allows better opting out of the recording. (For instance, if the parents do not want to be tracked at some point, they can turn off their microphones.) This will result in a multi-track recording, like with the babylogger, which has 4 microphones (all in the same device). This may open up in the future interesting technical approaches to separating sources and removing noise from the recordings, although to our knowledge no speech technology team is currently working on this. 1.3 The bottom line regarding hardware Let me just say that there is no combination of hardware that is perfect and everyone should use, so the choice really depends on your particular setting. I am going to talk about some general purposes that could help you make an initial decision. For most people, we recommend LENA. In particularl, if you are someone who doesn’t have a lot of technical background and you don’t have a tech support in your institution, you don’t have mathematical knowledge or programming knowledge, you are not particularly interested in very fine details of what happens in the recordings and you don’t have a lot of support to do human annotation, then probably the best choice for you would be a LENA product. Why? Because LENA has been used very widely, in many different labs and your data would be comparable to many other people’s data. Also, it requires very little of you in terms of technical skills, it’s very easy to use, and it comes with a large community of users who could help you in many different ways and you could also cite a number of papers on how technical decisions have been made. If you have some programming knowledge, and like tinkering with tech, then Babyloggers or the wireless systems are probably your best choices, provided you have the budget for them. These approaches have the most flexibility and richness, allowing you to contribute to the development of software that exploits these capabilities. If you are doing field work in remote sites, our preference for this are the USBs. All of the equipment suffers somewhat in hot and/or humid conditions, and in the presence of dust and/or salty air – in this setting, having a device where each unit is very cheap really takes the pressure off. They also consume the least electricity and are easiest to charge than all other options. Also, security hazards of someone getting access to the recorded data are not likely if there are few working computers where you are. Please note we have a dedicated video with tips for people working on field conditions – see Chapter 19. If you are doing large-scale studies, with hundreds of families, we’d also recommend USBs because they are the only option at scale when you have a smallish budget, and because experience shows that you’ll always lose some devices in this setting. That said, you need to put extra care in talking to everyone involved in order to make sure that there is no misappropriation of the data. If you are doing a longitudinal study with a small number of families who are strong partners in the research, for instance with a minority community or in a multilingual setting, the Babylogger may be ideal because of the on-device encryption and the very large amount of data that can be held. This will allow families to record one day a week for several weeks, without having to interact with you to off-load the data on a computer. If you are interested in phonetic or acoustic studies and need high quality recordings, then hand-held recorders or a wireless system will be ideal, because you have actual control on the recording quality. In all other cases, you do not: the recording quality is set by the device. If you are studying social interactions and want to make sure you capture interactions between the child and each and everyone else, then the wireless option is ideal, because no other one will capture in high recording quality with a near microphone everyone around the child. In our previous data, the voice of adult males and other children is much lower in intensity than that of female adults – we are not sure why that happens (perhaps young children tend to stay close to female adult carers), but it does make us worry that the other devices, with their low resolution, may not capture equally well the speech of these other partners. 1.4 Summary Hardware Ease of use Cost Length of one bout Storage capacity Childproof Pros Cons Best choice for LENA easy 400US$ 24h 72h yes accurate internal clock / For most people iPods not easy ? ? ? no collect data intermittently, recordings are time-stamped pricey / Olympus not easy for parents 200/300US$ 22h ? no better audio quality than LENA, higher sampling frequency and a flat frequency response rate heavy, data could be easily stolen, not accurately time-stamped If you are interested in phonetic or acoustic studies USB easy 10/20US$ 15h depends on the brand not completely: caps could be choking hazards technical audio quality is comparable to those of the other devices, cheapest device sometimes recording fails, data could be stolen If you are doing field work in remote sites, or large-scale studies Babylogger easy 600€ 24h a week yes molded on LENA but has 4 microphones instead of 1 and an accelerometer, encrypted recordings, best storage still in piloting stage: expensive and sometimes has issues If you have some programming knowledge, and like tinkering with tech, or if you are doing a longitudinal study with a small number of families Wireless systems it depends ? quite long ? not necessarily Quite cheap, can be easy to use, causes less privacy problems beacuse it doesn’t record outside the child’s home, has better audio quality you need to set up quite a bit of equipment in the child’s home, If you are interested in phonetic or acoustic studies, or if you are studying social interactions 1.5 Resources LENA overview LENA device specs iPods Sample hand-held recorder Sample USB recorder please note we have not tested this one! so do buy a sample one before you buy them in bulk. There may be others that are shipped to your location. Babylogger Sample wireless system: microphone please note we have not tested this one! Also, this is just the microphone with a transmitter – you’ll still need a receiver in the child’s home. "],["add.html", "Chapter 2 Additional measures 2.1 Movement 2.2 Visual information: Snapshots 2.3 Visual information: Continuous recordings 2.4 Other information 2.5 Resources", " Chapter 2 Additional measures As we said in our introduction video, we are not going to talk into detail about how to collect and analyse data that are not audio, but we do want to have some reference for people interested in this work. There are some people who already have added other measures: 2.1 Movement One case where another measure is integrated in the device is the Babylogger, which has an accelerometer. I (Alex) personally haven’t analysed those data, but in theory this could be useful if you want to look at changes in how children move (i.e. if you want to identify walking or crawling). To my knowledge, there aren’t yet automated algorithms to do this, so you may need to do some manual annotation and algorithm development. 2.2 Visual information: Snapshots If you are interested in visual information, Marisa Casillas has used photo logging. She used a device that is no longer purchasable, that is called Lifelogger which took pictures every some numbers of seconds. Marisa has published two papers where she shows pictures taken by the device, so you can check them out – see links in the resources. She also added a fish-eye lens, because she wanted to get more of the visual environment and, to my knowledge, she has quite a lot of hand annotations of this; also, she has some explanatory work on how to automatise the analysis, but it is still to be done in the future. 2.3 Visual information: Continuous recordings Some of you want to have videos to capture gestures and other things that cannot be captured by pictures. I know that many people are really interested in this, but this can also be really challenging. Why? Collecting videos requires a lot of energy and batteries make the hardware heavier. In the case of videos, the battery just runs out very quickly, so it’s not something that you can collect over an extended period of time – i.e., hours at a time. So it’s technically impossible to collect 10h of video with a device that is wearable by the child. That said, what people are doing is that they are collecting videos separately, for a short period of time. An example would be the work of Elika Bergelson: she collected daylong audio recordings for a month for each child, and on a separate day she went back to the children’s homes, set up a tripod and had the kids wear a cap on which she had mounted two video cameras. This way, for each child, she has some video data and some audio data. So if you are interested in that approach, you can check out her work from the references. 2.4 Other information There is another set of analysis and possibilities that come from Kaya de Barbaro’s work. She has a few papers describing a technology that she and her team are developing and it’s really interesting because it contains many psycho-physiological measurements including for instance children’s heart rates. She also uses parental questionnaires on a phone app, which asks parents questions throughout the day. This is another type of work that it’s beginning to be explored, so I encourage you to check out Kaya de Barbaros’s work for more information on that. 2.5 Resources Casillas, Marisa, Penelope Brown, and Stephen C. Levinson. “Early language experience in a Tseltal Mayan village.” Child Development 91.5 (2020): 1819-1835. pdf Casillas, M., Brown, P., &amp; Levinson, S. C. (2021). Early language experience in a Papuan community. Journal of Child Language, 48(4), 792-814. pdf Bergelson, E., Amatuni, A., Dailey, S., Koorathota, S., &amp; Tor, S. (2019). Day by day, hour by hour: Naturalistic language input to infants. Developmental science, 22(1), e12715. pdf de Barbaro, K. (2019). Automated sensing of daily activity: A new lens into development. Developmental psychobiology, 61(3), 444-464. pdf "],["clothing.html", "Chapter 3 Clothing 3.1 Custom-sized pockets 3.2 Alternatives to t-shirts 3.3 Final comments 3.4 Resources", " Chapter 3 Clothing In this video we are going to discuss clothing. Let me start by answering something I’ve been asked many times: you cannot have the recording hanging from a string, or in a pouch, or in someone’s pants’ pocket. All such recordings will have noise that will make automated analyses impossible. Your only option is to have the recording worn 10-20cm away from the child’s mouth, as stable as possible with respect to the child themself. This means one of two things: either in a custom-sized or adapted pocket, or clipped to the child’s outermost layer of clothing. We speak about each in turn. 3.1 Custom-sized pockets 3.1.1 LENA t-shirts If you are planning to use the LENA device, you are probably going to buy the clothes they are selling, usually t-shirts. These t-shirts are excellent, like the device, but like the device they are also quite pricey (25 US$ per unit). If you have chosen babyloggers, we also recommend LENA clothing – we explain why in a moment. If you don’t buy LENA t-shirts, you have a few other options. 3.1.2 Non-LENA t-shirts First, you can try and replicate what LENA decided, meaning having t-shirts with a pocket sewed on them. In our experience, buying 100% cotton t-shirts and having pockets sewn on them is going to cost you more or less the same as buying LENA t-shirts, with a lot more hassle, since you need to find someone for the sewing, send them a prototype, etc. If you are using LENA or Babylogger as your device, then you’re probably better off buying t-shirts from LENA. However, you may have no choice than custom-made clothing. This could happen because you work in conditions where t-shirts are not ideal - perhaps it is too cold, with children outside most of the time wearing several other layers of clothing, in which case the recording will be muffled. It may also happen because you have chosen the USB system or a hand-held recorder – these devices require pockets that are a different size from LENA’s device (and pocket). In this case what we suggest is that you make pockets where the device can fit perfectly snugly, so that it doesn’t move while recording and this avoids recording too much noise. The pocket must be really very, very tight, so that there is absolutely no friction between the clothing and the recording device. Additionally, the t-shirt itself should be tight against the child’s body, and not hanging loose. Second, you can buy a t-shirt that already has a pocket, even if the pocket is not the right size, as long as you stuff something in so that the device itself doesn’t move. You can use fabric, or cotton – whatever you use, just consider that the family should be able to retrieve the device to turn it off and put it back in after turning it on, without causing bad quality data. That is, parents may put the stuffing in front of the device rather than behind it, so that the sound is muffled. Additionally, you need to consider whether the stuffing may present any kind of hazard to the child. 3.1.2.1 How to close the pocket When having the t-shirt made, ask the sewing person to sew a pressure button on the pocket, like the ones LENA t-shirts have. This person will probably complain about this because they are hard to make if you do them by hand. But there is really no alternative to these pressure buttons because they are both childproof and easy to use by parents/researchers, plus they make sure the device doesn’t fall off. One alternative we used in the past is velcro strips. These are fine if you work in an urban setting, which is very clean, and if you are not going to use the t-shirts over and over. We used them in a field setting and we were not happy with them. To begin with, dust and/or sea air caused the velcro not to stick. Also, as we washed them, they became less adhesive. The last option you have for closing the pocket is a regular button with a button hole. You should think carefully before pursuing this, because if the children you are going to work with are a certain age (old enough to play with the button, but young enough to not realize the risk), small buttons can be a choking hazard. 3.2 Alternatives to t-shirts If none of the options above are good for you, you have some more. 3.2.1 Vests If you are going to work with populations where t-shirts are just the lowest layer (because they live in cold places), then t-shirts are simply not a good idea, because there are always going to be more clothes on top of the recorder and this causes noise and muffled sound. An excellent alternative are vests. LENA does not sell them anymore, but you can ask around and see if someone still has them because they are really terrific: the child can wear them as inner or outer layers of clothing, so they can be a good solution for moderate and cold climate. 3.2.2 Harness How about places where it’s so warm that children often do not wear any clothing? A possibility is that you create a system of straps that kind of looks like a harness (or a bra). One researcher that uses them is Marisa Casillas. We give a link to her paper in the resources section of this video. The paper contains a picture of her harness: it has a pocket on the front where you can put the device and elastic straps, to fit children of different ages. It can be used in very warm climates, as the only layer, and very cold climates, as the outermost layer. 3.2.3 Attaching the device to a piece of clothing Another option is to buy a money clip or a very large paper clip, so that you can attache the device onto the child’s t-shirt or sweater. If you do that, remember to check whether there is something in this system that might cause a choking hazard: the clip needs always to be childproof and child friendly. Remember also that families need to access the on/off button to make use of their right to withdrawal. 3.3 Final comments I’ve already said this but I cannot repeat this enough: To ensure the best quality audio, Try to use clothing where the device is completely stable (tight-fitting clothing, device firmly attached): you want to avoid noises from clothes rustling as much as possible; Place the recorder in the outermost layer of clothing possible to avoid muffled sounds. 3.4 Resources LENA t-shirts: LENA no longer has a section where you can view their products. You should approach them for a quote via the LENA contact form Custom pockets: We don’t yet have a publication for this. Harness-like: Casillas, Marisa, Penelope Brown, and Stephen C. Levinson. “Early language experience in a Tseltal Mayan village.” Child Development 91.5 (2020): 1819-1835. pdf "],["lena.html", "Chapter 4 LENA software 4.1 Resources", " Chapter 4 LENA software In this video, we are only going to discuss the LENA software. We discuss alternatives to the LENA software in another dedicated video 5. Let us start with how the procedure goes for LENA. Once you have done your recording, you can connect the device to a computer with access to LENA-licensed software, that automatically extracts and analyzes the recording. There are two licenses allowing you to keep a copy of the recordings: “Pro” and “SP”. I really do not recommend buying the licenses that do not allow you to keep a copy of the recordings, because, as we discuss later, LENA is no longer state of the art, and the metrics that can be extracted are limited. So I won’t be discussing other alternatives. So on to these two alternatives: The Pro system does all speech processing in the local machine, which needs to be running Windows 7 or 10; typically, you can only install the software in one machine. The SP license accesses LENA’s software via an Internet connection, which may be an important advantage for studies spread over multiple locations, since you would be able to securely upload, process and inspect results from any Internet-connected computer. Through this step, you extract the audiorecording from the recording device to “clean it” and allow another recording to take place. In addition, this automatically launches the analysis of the data through the LENA software. Once the analysis is complete, the LENA software provides several types of automated annotations. You will be given a file in which the audio signal has been classified into different types of classes (for example you will have: key child, adults, TV, etc.). Also, the segments tagged as belonging to the key child, will have an estimate of what portions are vegetative (e.g., burps) or crying, as opposed to speech. In recent versions of the software, speech-like vocalizations are also tagged in terms of the number of canonical sounds they contain. Finally, for each stretch of speech tagged as being produced by an adult, there will be an estimate of the number of words spoken in that stretch. At a second level, the LENA software also derives a few descriptive statistics that are averaged over five minutes, one hour, and the whole day. In addition, the system also provides an estimate of child vocal maturity compared to other children of the same age and sex using a standardized score called Automatic Vocalization Assessment (AVA). If you are interested in collecting language properties such as measures of lexical diversity, syntactic complexity, and who is being talked to, you need to know that the LENA system doesn’t provide automated analysis for that. (Incidentally, no current software does this yet.) Also, the LENA software is in principle most accurate for children learning American English aged between 2 months and 3 or 4 years, which is the population used to train and test the analysis software. As with any tool, if you deviate from the population on which the tool was developed and validated, you should take care to test the extent to which the measurements are still reliable. We don’t mean to scare you by saying that, and you may be surprised since you have probably heard that “LENA has been validated in many languages”. We have reviewed carefully this previous literature, and indeed, it is the case that people working in a wide range of languages have looked at the accuracy of LENA. That doesn’t mean that accuracy is perfect everywhere! So the phrase that “LENA has been validated in many languages” is not perfectly accurate, it should be instead “there have been checks of LENA validity in many languages”. We’ll see in a different video how well the LENA software fares with other languages but in a nutshell: pretty well in many, very poorly for some speaker types in some languages. So if you are working with a population that speaks one of these languages, the LENA software might provide you with inaccurate or inappropriate measurements and if you are working with a language speaking population that hasn’t been validated, then you need to do the validation yourself – we cover all of this in later videos. The LENA system’s automated estimates are most reliable if the wearer is recorded for 12 or more consecutive hours within a single day, though the device can accommodate recordings being split over several days. 4.1 Resources LENA SP There is currently no link explaining LENA Pro, but if you are interested in it, you should approach the LENA Team and ask them about it. "],["nonlenasoftware.html", "Chapter 5 Non-Lena Softwares 5.1 Resources", " Chapter 5 Non-Lena Softwares In this video we’re going to explain how to use a software that is not LENA in order to do your analysis. One important point I want to make right off the bat is that you need to know a little bit of coding to use this software so if you have never used a terminal and if you don’t know what bash or Python are then you probably want to get some training on that first. We provide some links for this in the further resources section of this video. We are going to base our explanations on the software that was developed in the ACLEW project, where the analysis was broken down into several phases. The first phase involves deciding who speaks when. For this phase you need to use VTC, short for Voice Type Classifier. VTC was trained on a large corpus of over 200 hours of child-centred recordings that were put together by combining some data that we had in our lab with data available on CHILDES. Most of the data actually came from CHILDES, in particular from the Tsay corpus that contains samples from Chinese language. The rest of the data comes from several different languages including English, French and other languages from Oceania, America, and Africa. So one big difference compared to Lena is that it was not trained solely and exclusively on children learning English. The second phase of analysis applies only to sections that the Voice Type Classifier identified as being adult speech. For this we use another piece of software that’s called Alice, short for Automatic Linguistic Units Counter. Alice was also trained with multiple languages although much of the data was English from the US and from the UK. Both of these are open source, which means that you can download them and reuse them – you can even change them anytime as you think best, since both of them can be retrained. Even if both of them can be retrained, you can also use them out of the box. Both of these tools have been benchmarked against LENA and both of them are competitive. For more information, see the 14 Video. 5.1 Resources Lavechin, M., Bousbib, R., Bredin, H., Dupoux, E., &amp; Cristia, A. (2020). An open-source voice type classifier for child-centered daylong recordings. Interspeech. pdf code Räsänen, O., Seshadri, S., Lavechin, M., Cristia, A., &amp; Casillas, M. (2020). ALICE: An open-source tool for automatic measurement of phoneme, syllable, and word counts from child-centered daylong recordings. Behavior Research Methods. pdf code "],["childproject.html", "Chapter 6 Tutorial of our ChildProject software 6.1 Resources", " Chapter 6 Tutorial of our ChildProject software The ChildProject package is one part of a three-part solution to the difficulties faced when working with long-form recordings. The other two parts are Datalad and git-annex, which we do not detail here. ChildProject is a package to manage daylong recordings. It is written in python, but it can also be used from the command line so that no knowledge of python is required. However, notice that this does presuppose that you know your way around a terminal – if you have no idea what that means, we strongly encourage you to take the online free tutorial by Software Carpentry in the resources section. As we said in another video, people who know another computer language (like python or R) learn bash basics in less than an hour. And if you don’t know other languages, it may take you 3-4 hours, so it’s still not very time consuming. Within ChildProject, we have defined standards regarding the structure of the data, which we explained in previous videos. The package helps enforce these standards with tests. It also provides tools to convert annotations from a variety of input formats (LENA, ELAN, Praat, ALICE/VTC, etc.) And, it has more functionalities, including batch audio conversion, sampling of the audio data for annotation, generation of files for annotation using ELAN, anonymization of LENA’s .its files, and more! All of this power comes from having defined very clear standards as to how your dataset is structured. This means that you need to organize it in a specific way. We explain how to organize your data in the Video 10. With ChildProject, you can check whether you have actually organized correctly by running a simple command: childproject validate. The package also provides some commands to import annotations into a standardized .csv format. With these commands, you can get the package to convert raw annotations (from humans, LENA, etc.) to CSV dataframes, and index them in metadata/annotations.csv in a way that makes it easy to then do other manipulations, like finding which sections of the audio are in common between two human annotators, or between LENA and a human annotator, for instance to calculate reliability. Another reason to adopt our framework is not just that you’ll be able to perform such computations, and enrich your dataset, but also that we give you a framework in which you can keep track of the changes you make to your dataset. For this, we use DataLad. DataLad is a decentralized system for integrated discovery, management, and publication of digital objects of science. It builds on git-annex, which builds on git. In a nutshell, git is a system for version control, which keeps a history of all the files you put in a place and all the changes made to them. If you’ve used the history in Google Drive or Dropbox, it’s a bit like that, except that you have more control over how the versions are defined, and you can add comments as to how they differ from previous ones. One limitation that git has is that it does not handle large or binary files very well – it has a hard time finding changes in these kinds of files. So git-annex is an extension of git that handles large files. DataLad is built on git-annex, and adds abstraction and more features, like analysis reproducibility, dataset nesting, and others. We won’t cover all of these in this series because once you can understand the principles of dataset nesting, you’re ready to take on purely written instructions. So for these more “advanced” operations, we recommend checking the ChildProject documentation and the Discussion section, both of which are linked in the Resources section. One last thing: if you use our framework, you will also find really easy-to-follow instructions to create a private archive on GIN, the G-node infrastructure, which is a university-led project trying to help neuroscientists use modern data management techniques. We have been lucky enough that they have allowed us to host our datasets with them. If these grow very very large, we may need to pay some costs. But for now, it is an amazing resource because it has a git-annex-compatible interface, the servers are in Germany, and are GDPR-compliant. 6.1 Resources ChildProject documentation ChildProject discussion board ChildProject documentation on datasets structure Gautheron, L., Rochat, N., &amp; Cristia, A. (2021). Managing, storing, and sharing long-form recordings and their annotations. pdf "],["piloting.html", "Chapter 7 Piloting 7.1 Resources", " Chapter 7 Piloting In this video we are going to talk about the importance of piloting. You might think that since people have already done these kinds of studies in the past, then you don’t have to do piloting yourself. I’m sorry to say – this is simply not true: You need to make sure that the community with whom you are working approve of your methods, feel comfortable, and have a chance to give you feedback. What do we mean by community? We mean a superset of people you are going to sample from. This depends on your study. Often, this is defined by your inclusion criteria; and sometimes also by how you recruit. For instance, if you intend to have a sample that is representative of the population of a given city, this will mean trying to reach out to parents with a wide range of educational backgrounds. Or in other cases your project will call for a more homogeneous sample, perhaps including only parents who have a lower level of education. In any case, you should contact at least 2 families who come from the population that you are going to sample. Note that in some cases, this may exclude your friends or family. You can start early piloting by asking friends and family, but if they do not belong to the community you aim to sample from, then their answers might be different, and therefore misleading for you. What do you want to discuss with the community? You can start asking them how they feel about the clothing – some communities prefer clothing that is flashy (so that everyone knows which child is being recorded) and others prefer clothing that “blends in” (so that the child does not feel singled out). Also, you can ask about the device you intend to use. We’ve seen communities tend to have strong preferences about the device they prefer their child to wear. Some families don’t like the USB device because they feel like it can be easily lost, resulting in an additional worry for the parents. Other families have a preference for the USB device, especially when used for extended periods of time and during the night, because they think the child will be more comfortable with this lightweight option. Ask also for their opinion on how you aim to analyse their recordings. It’s always very useful to talk about this process with the families, and get their impressions on what it means when manual as opposed to automated methods are used. We gave found that some communities do not like the idea of human annotation: they worry about what the people who listen to the recordings are going to think of them. This is something you can address in your consent form and/or other explanatory material. For instance, you can specify that listeners are going to be professional and respectful about it, and act with awareness of the unique situations each family is in. Some of the complaints and worries we got about annotation are concerned with automated annotation, but they are very very few and usually they come from very informed parents that come from the tech industry. In this case, parents might know the use of automated analysis for purposes that are close to surveillance or that are being used in discriminatory manners (e.g face recognition, since algorithm are trained with faces taken from the internet, which is biased with faces of White people, these algorithms might work against people who are not white). I personally am concerned about “algorithm bias”, the possibility that an algorithm systematically misrepresents data from one type of community. So I think it’s worthwhile to still bring up the topic of automated annotations and ask families what they feel about it, so you can make sure to address this in your consent form or other explanatory material, as well as in your annotation process (for instance, by retraining the software). The next thing that should definitely be among the topics that you bring up is mandatory reporting. Some of you will live in a State or Country where you are compelled to report anything illegal (e.g drug consumption); any suspicion of child-directed violence; or any suspicion of developmental delays. If this is your case, first ask yourself is something in your analysis process could reveal one of these cases. If you are relying purely on automated processing, then you probably won’t be able to find illegal events or violence, but you could find evidence suggesting a developmental delay. If you are using human annotation, all three could be discovered. Next, make a plan about how you are going to report it. Finally, tell your pilot families about this, and ask them what they think about this plan. Remember that since you will have to specify this anyway in your consent form, it is important to know whether this will result in certain families opting out of the study as a result, and therefore your study ending up with a biased sample of the community you were hoping to represent. What else should you ask? Honestly - everything! It’s always good to involve communities in every step of your study, which will be enriched by this contact. So take a moment to discuss all the stages of your study with the representatives of the community you are working with, because you want to minimize the surprises you find down the line. Start with what the goals of your study are, asking families what these goals mean to them. Sometimes when you have a very narrow, technical goal, parents might not really care about it, and that’s fine. You just want to check if they have any strong feelings about it. A related question is if there are other areas or research goals they want you to be thinking about. For example, something that happens very frequently is that certain communities are interested in research about bilingual or multilingual children development. Some communities speak a language that is not spoken by many and they are afraid their children will lose it. Yet another question they may have is how this participation will help their children’s academic outcomes. This doesn’t necessarily mean that you’ll change your research goal for this study, but it could help you strengthen your research by incorporating these ideas in a future study. Or if you are well connected in the community, perhaps a community member will be interested in exploring these idea via data reuse – as we just discussed. Piloting is a the natural time to look into this, so that you can already foresee it in your ethical request. When talking about goals with the families, you’ll also want to discuss about what could happen in the context of data reuse. We will talk about data reuse in another video, but we think that these data are really valuable, so they shouldn’t be used for a single purpose, right? People have gone through a lot of effort to collect them (including the participating families), so they should just be thrown away after one use. Piloting is a great time to talk with families about the options for donating data and reusability of data – since this needs to be specified in your ethical request, and in consent forms and explanatory material. When having these discussions, don’t think just of your research interests. We recommend that you also mention other work people are carrying out with these studies, such as automated recognition of child and parent emotion, computational modeling of language learning (or training of artificial intelligences), etc. You can then make your questions more precise, and see if parents are ok with data being reused in different settings. Finally, this is the perfect time to ask families (and the community as a whole, if this is relevant) about the feedback they want, whether they are interested in having some of your resulting data and in which form. You can then see whether this is logistically and ethically possible for you to do. For instance, we do not recommend sharing back the whole recordings with them unless you know for a fact that third parties are not recorded anywhere – and even if it’s only the family, as these data could be e.g. used in divorce cases (which was not a purpose mentioned in the consent form). So keep an open mind, trying to accommodate the families’ wishes, but also being careful about the ethical and legal implications of such choices. 7.1 Resources Cychosz, M., Romeo, R., Soderstrom, M., Scaff, C., Ganek, H., Cristia, A., … &amp; Weisleder, A. (2020). Longform recordings of everyday life: Ethics for best practices. Behavior research methods, 52(5), 1951-1969. pdf Levin, H. I., Egger, D., Andres, L., Johnson, M., Bearman, S. K., &amp; de Barbaro, K. (2021). Sensing everyday activity: Parent perceptions and feasibility. Infant Behavior and Development, 62, 101511. pdf "],["irb.html", "Chapter 8 IRB 8.1 Basic information for all IRB submissions 8.2 Additional considerations 8.3 Text you can use in your IRB requests: 8.4 Closing thoughts 8.5 Resources", " Chapter 8 IRB When you are considering doing research involving children, you will need the approval of the Institutional Review Board (IRB) of your institution. If this sounds unfamiliar to you, you can start by looking over the Wikipedia entry for IRB. We will not be covering the basics of IRB requests here, but instead, we’ll focus on information directly relevant to collecting long-form recordings. 8.1 Basic information for all IRB submissions When you specify the methods, you can state that recordings are gathered using a device that is attached to the child’s clothes, followed by automated annotations with speech technology tools. Include in your request information about how data will be transferred from the device to a storage unit, and from their to at least one back-up not in the same site. See the Storage video for more information 11. We strongly discourage you from saying that you’ll collect the audiorecordings, analyze them, and then destroy them. This is because some of the risks that long-form recordings entail (security, discomfort at thinking about being recorded) have already happened, whereas several benefits can ensue from continuing to store and analyze the recordings (provided storage is safe and analysis is respectful, of course). Consider also whether recordings will take place exclusively in the child’s home, or whether participants will also leave the home with the recorder on. In the latter case, your IRB may require you to provide evidence that you have checked that incidental recording is not against the law where the family lives. If it is legal, consider also how these data are going to be handled. Some researchers ask their participants to tell these others to get in touch with the researchers if they have questions. Others ask anyone who comes into contact with the device to read and sign a consent form. You may be asked to specify why you are collecting the long-form recordings (e.g., collecting infant speech, investigating parental behavior, etc.) When doing so, please consider the fact that, when properly handled, these data can be incredibly rich and inform a variety of questions, all the while protecting the rights and well-being of participants. So unless there are strong reasons for you to specify a very narrow objective, then it is ideal to mention that recordings will be used for your research project as well as archived in scientific repositories for potential re-use for other research projects. It is customary to include in an annex the actual consent form, or the consent speech used to explain the procedure to participants. 8.2 Additional considerations In some cases, it is desirable to have humans annotate part of the data, to check accuracy of the algorithms and/or to help develop additional algorithms and/or to complement with more qualitative small-scale approaches (see Annotations video 15). If you are planning on doing this, remember to mention this in your IRB request. Consider the training of annotators (including on the ethics of recordings) as well as how you may deal with unexpected discoveries (of illegal acts, child abuse, etc.) In addition to in-lab annotations, you can also opt for citizen-science annotations. You can see an examples of this technique here. In that study, we preserved anonymity of participants by extracting clips that were 500 ms long, and presenting listeners with clips extracted from many different children. This was possible because the decision we were asking for was very simple and could be taken with local information (e.g., who is talking: a baby, a child, or an adult?) An alternative that other researchers have selected is to have someone in the lab listen to sections of the audio and determine that there was no compromising information shared, and then these clips were annotated by citizen scientists for higher level content, based on listening to a whole sentence or even a whole conversation. If the latter seems interesting to you, then just make sure that your participants agree to this, since clips thus shared on Zooniverse become public. 8.3 Text you can use in your IRB requests: 8.3.1 Basic description Participants will be asked to wear a &lt;INSERT HERE NAME OF DEVICE (e.g., LENA, USB)&gt; in . The device records audio for up to 16 hours . Participants will be explained how to turn it on/off in order to use their right of withdrawal. In addition, they will also be able to tell us if there are sections they do not want included in the data and/or erased. Recordings will be stored in a safe location, only accessible to individuals with appropriate training for such sensitive data. 8.3.2 Automated analyses Given the length of these audios, they will not be studied in their entirety by a human researcher. Instead, they are processed with an automated system that assigns sections of the recordings to the child wearing the device, other children, male adults, or female adults. Additional automated processing classifies vocalizations as child-directed or overheard by the child (i.e. spoken to someone else); estimates the number of sounds, syllables and words in a turn; and/or decides whether a given vocalization is crying, laughing, or other vocal types. Thus, the content of what is said is not inspected. 8.3.3 Manual annotations for validation To check that automated analyses are valid, sections of the audio will be extracted and human-annotated. Annotators will have training allowing them to treat these confidential and sensitive data appropriately. They will use a computer program to indicate who spoke when and to whom, transcribe what has been said to estimate the number of sounds, syllables, and words, and/or decide where whether a given vocalization is crying, laughing, or other vocal types – that is, the same levels that are used in the automated analyses. &lt;DESCRIBE ANY ADDITIONAL PROCEDURES YOU FORESEE, LIKE REPORTING OF ILLEGAL ACTIVITIES, VETTING SECTIONS WITH SENSITIVE INFORMATION, ETC&gt; 8.3.4 Manual annotations to augment dataset To complement automated analyses, sections of the audio will be extracted and human-annotated. Annotators will have training allowing them to treat these confidential and sensitive data appropriately. They will use a computer program to . 8.3.5 Manual annotations using Zooniverse – general Zooniverse is a citizen science platform with hundreds of thousands of users in the world. It has led to significant discoveries, as individuals can contribute their time and expertise by annotating data in their free time. We created a project on Zooniverse that was approved by the Zooniverse board and released in February 2020. This project asks citizen scientists to help us classify children’s vocalizations in different types (crying, laughing, and two types of speech-like sounds differing in their maturity level). To this end, daylong audiorecordings are algorithmically processed to detect sections that contain speech. These sections contain identifying information (people’s voices) and may contain sensitive information (e.g., people’s names, intimate moments). 8.3.5.1 Section to add if using short clips Therefore, we don’t have whole sections annotated, but instead cut these sections up into smaller clips with a procedure we explain next. Once the clips themselves are uploaded onto zooniverse they cannot be protected - they are on the web so they can be scraped. What we do is we render scraping uninteresting in several ways. First, we make the clips very short. In fact, we now extract .5s clips, so a 1.3s section attributed to the child will be represented by 3 x 500ms clips (adding some extra audio on the edges). So now, even if someone ill-intentioned gets some clips, they cannot guess that a 300ms one was probably part of a bigger one — all the clips are the same length. Second, we ramp intensity in and out of each clip throughout the first and last 5 milliseconds of the audio, which is enough to destroy the amplitude of the signal, a cue ill-intentioned people could use to “glue” clips together. Third, we upload many clips, from children of different ages, and different languages, all mixed together. So if anyone goes through the trouble of scraping them, they would face an incalculable problem of trying to determine which bits belong to the same section, the same recording, the same child. 8.3.5.2 Section to add if using vetting Therefore, before uploading these audio sections, trained personnel will listen through and make sure that there is no sensitive information. Parents will have been informed about the fact that their voice may be recognizable and that clips enter the public domain, and will be able to opt in or out from this part of the research in the consent form. 8.4 Closing thoughts This is a very short introduction to this important topic because there are already excellent resources available on this. Check out the resource section for links to consent forms examples, an example of a Data Protection Impact Assessment, and more information. 8.5 Resources You will find sample consent forms from several labs on https://osf.io/d4tcu/ In particular, if the reuse for ExELang would be after you archive the data in a scientific archive, you can simply have a section where you ask whether they want to opt out of data donation - https://osf.io/5e3vz/ has an example of an opt-out check-box under point 2 If you’re uncertain about archiving, you can instead use the language under point 11 of the same consent form, the option is “Checking this box indicates my permission to store, use, and share my information from this study in research databases or registries for future research conducted by the current investigators and their research partners.” Data Protection Impact Assessment (DPIA) for ExELang Project link For more in-depth information on this topic, see: Cychosz, M., Romeo, R., Soderstrom, M., Scaff, C., Ganek, H., Cristia, A., … &amp; Weisleder, A. (2020). Longform recordings of everyday life: Ethics for best practices. Behavior research methods, 52(5), 1951-1969. pdf Casillas, M. &amp; Cristia, A. (2019). A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings, Collabra. See Appendix A “Further considerations for ethics and permissions in sensitive contexts” "],["logistics.html", "Chapter 9 Logistics 9.1 How to deliver the hardware 9.2 How long to record for 9.3 How to communicate about the hardware with family members 9.4 How many devices do you need? 9.5 Resources", " Chapter 9 Logistics In this video we are going to talk about logistics, including how many devices you need for your research and how to organize your data collection. We are going to focus on an urban setting because field conditions are discussed in another video (see Video 19). 9.1 How to deliver the hardware Many researchers in the past would go to the family’s home at the beginning of the day, to fit the child with the device and turn it on, then picked up the device at the end of the day, so that the parents didn’t need to do anything particular. I think that today, whether you are using LENA or another device, you don’t need to drop off and pick up the equipment yourself. There are downsides to doing the drop-off and pick-up yourself in the same day of recording, including that your presence is very salient to the child and the people around them, and also that you are not capturing interactions early in the morning and late in the day. You can instead mail the device and clothing to the families or use a courrier service. Even if you decide to visit the family, you can explain to them how to turn the recording on and off – which they need to know anyone to apply their right of withdrawal (i.e., to not have certain sections of the day recorded). Another option that may be good for you depending on your community contact is to have devices and clothing delivered to a trusted center, like a daycare or a clinic – provided that you can make sure that nobody will tamper with the recordings and that the devices will be kept in a secure location. In the hardware section, we explained that the only device for which you have time stamps with respect to local time is LENA. So if you are using a different device, you should ask parents to speak into the device, after it has been turned off, what time it is then. This will allow you to track the time and position the recording with respect to time of day. You can ask them to again say the time into the device if they pause it. Please note that some devices do not start recording immediately – so to be on the safe side, ask the family to count to five before saying the time and day (i.e. say “one two three four five – today is Wednesday August 3 2021 and the time is 09:25am”). 9.2 How long to record for We recommend recording for the maximum storage duration whenever possible, because we have some initial evidence that derived metrics are more reliable the more recordings you have, perhaps because with more data are a better representation of what the child’s environment is like. (We also have some evidence that recordings about 4h in length or shorter have a lot more speech than longer ones – probably because the family is overly conscious – so try to record at least 6h continuously, without asking the family to interact with the recordings again.) If you are using the newest generation of LENA, that means you can record up to 72 hours total, asking families to recharge the device overnight. Similarly for the USBs, which can often hold 150 hours total; and the Babylogger, which can hold even more. That said, most researchers are only recording one full day, and as we will see below, instructions to parents do get more complicated the more you ask them to do, so think deeply about how much data you truly need. 9.3 How to communicate about the hardware with family members We have found it useful to provide parents with both oral and written instructions on everything they need to know: how the device works, how to turn it on, how they need to attach it to the clothing, how to recharge it (if you are recording multiple days), and that they can just go about their day. The written version could be provided printed out as well as on a website. This is useful because you can also have FAQs at the end, which will be composed by the things that the representatives of the community told you during your piloting sessions (see Video ??. This may be how the data are going to be analysed, how they are going to be used for, etc. So anything parents told you or asked you about in the piloting stage, you can have it in a little booklet. The booklet is useful to some parents who like to consult it and have everything written down. However, for some things, it may be better to have short videos that are accessible through a QR code that the parents can scan in the booklet. This way, they can read the explanation or watch the video. Particularly, regarding how to put the device in clothing, the booklet has pictures but nothing really replaces a tutorial video, so if you can do that, it will probably be appreciated. Ideally, you should also have a phone number they can call or send a message to, for instance via WhatsApp, if that is common in the community you are working with. This is very reassuring in case they have questions that are not in the booklet, or they want to refer someone else to you, for instance because they were incidentally recorded. 9.4 How many devices do you need? This does not only depend in the number of families you are recording, but crucially on the logistics, and how devices get dropped off and picked up. Let us start assuming that you visit the home for drop-off and pick-up. So from the time you drop the equipment to the moment you pick it up, it is minimally going to take 3 days, because you will drop the device the day before they record and you’ll pick it up the day after. Also, we noticed that it’s not always easy for parents to launch the recording, and also, depending on your design, you might want to pick one week day and one weekend day, so in this case you might assume that one device is going to stay with the family for one whole week. Next, let us assume that you mail the devices, and have parents mail them back. In our experience, you should count a whole week on each side of the recording, because it is hard for parents to remember to mail it back, and the mail service takes some time. In addition to the time of the recording and the delays of transferring the devices between your institution and the family, you also want to consider the time it takes to repurpose the device and clothing. Extracting the data takes only a few minutes, but whoever does this needs to make sure to be very careful in assigning recordings to individuals, since an error at this point is hard to undo. In addition, you need to recharge the devices, which takes about 2h for any of them. If you are using t-shirts or other such clothing, you should also budget time for washing them. 9.4.1 Sample calculation 1 So, for instance, imagine you want to launch a study where you have 40 families participating and you want each family to contribute with one set of recordings per month from the ages of 6 months to the age of 12 months of the child. So in this case you will need at least 30 recording devices and 30 sets of t-shirts. This is because: imagine you have device 1 that goes with family 1 on week number 1. Then, you can use it with another family but probably it’s a little bit risky to use it on week number 2 so you will use it with family 2 on week number 3. This is because otherwise you would need to pick up the device, wash the t-shirts and drop the equipment off to the next family on the same day. (People who have been doing this in a large scale had cases in which a device got thrown in the toilet or it got crashed or the child jumped into a pool with the device on, so one should plan for at least one or two devices getting lost or broken). 9.4.2 Sample calculation 2 500 families, recorded twice, one year apart (pre- and post-intervention) study rolled out seeing 50 families in a given week 2.a. Assuming drop-off and pick-up through in-person visits to the homes: Given potential equipment loss and to reduce logistic burden, 60 recording units should be purchased (50/week + 10 extra). 2.b. Assuming drop-off and pick-up through mail (7 days to send out &amp; get back the recorders): Given potential equipment loss and to reduce logistic burden, 160 recording units should be purchased (50/week x 3 weeks given delays in mailing in/out equipment, 10 extra). 9.4.3 Sample calculation 3 50 families, recorded once a week (8h), for 3 months study rolled out by recruiting five families per month That means that at the “peak” of data collection, 15 families are recording the same week. Families keep the equipment over the 3 months of data collection, so drop-off &amp; pick-up happen only once per family. 3.a. Assuming drop-off and pick-up through in-person visits to the homes: Given potential equipment loss and to reduce logistic burden, 60 recording units should be purchased 20 recording units should be purchased (15/week + 5 extra). 3.b. Assuming drop-off and pick-up through mail (7 days to send out &amp; get back the recorders): Given potential equipment loss and to reduce logistic burden, 30 recording units should be purchased (10/week + (5/week equipment that is changing families) x 3 weeks given delays in mailing in/out equipment, 5 extra). 9.5 Resources Cost analyses for the ExELang project and partners, and note on benefits link Example information for parents (French) "],["organizingdata.html", "Chapter 10 Organizing your data 10.1 What information to collect about the children 10.2 What information to collect about the recordings 10.3 Resources", " Chapter 10 Organizing your data Longform recordings are rich data – but things can get messy if you don’t keep your files organized correctly. We strongly recommend following a file organization that will allow you to use the ChildProject package – we will explain the package in a dedicated video (6). More detailed explanations about this structure are available from the ChildProject documentation, but we provide here a brief and practical overview. There are at least three folders: recordings metadata annotations You can have more folders, and you can extend the logic of these three once you understand the underlying principles, which we will explain next. First, the recordings folder contains at least one folder, called raw. Inside it, if you are using USBs or other such devices, where the file names are not meaningful nor unique, we recommend having one folder per child per recording day, so that you don’t accidentally overwrite files or mix up which files belong to which children. If you are using LENA or Babylogger, file names are unique, so the danger is smaller, but you may find it useful to have one folder per child. We strongly discourage you from renaming files by hand, as errors can seep in. Second, the annotations folder contains one folder for each set of annotations. For instance, for LENA, you’ll generate an .its file for each recording bout; for vtc, an .rttm file for each recording file. Finally, the metadata folder contains tabulated data (excel or csv) that contains the list of children and their properties (e.g., age). There is another file that relates each file inside recordings to a child. And another that relates annotations to recordings – but this one is typically generated when you import annotation data, so you don’t need to worry about it. So what is most important for you to do is to keep a good record of all the features of children and recordings as well as of which file inside recordings corresponds to which child. 10.1 What information to collect about the children Collect at least: date of birth sex Many of our datasets also contain other information – you can read the list here. 10.2 What information to collect about the recordings date of recording local time in which the recording was started recording file name child ID Many of our datasets also contain other information – you can read the list here. 10.3 Resources ChildProject documentation on datasets structure Gautheron, L., Rochat, N., &amp; Cristia, A. (2021). Managing, storing, and sharing long-form recordings and their annotations. pdf "],["storingdata.html", "Chapter 11 Storing your data 11.1 Resources", " Chapter 11 Storing your data These data are precious and should be handled carefully, both to protect access and to make sure it is not lost. This will typically mean that once you have extracted data from the device, you should create a back-up. In our experience, it gets messy if you do a back-up by hand (through copy-pasting to another location), because you can make a mistake and replace a file, or forget to replace a file, and then you have copies that seem duplicates but you are not sure whether they are or not. Also, it is good practice for the back-up not to be in the same physical place as the main copy, as in case of fire, flood, or theft they may all disappear at once. Also, you should make sure people who do not have ethical clearance do not gain access to the data. Therefore, do not put the data in a cloud server (like Dropbox or Drive) with a link whereby anyone with the link can access it, since it means that if the link is found by someone, then they gain access to the data. Taking all of this into account, we believe a good solution is to use a system like Dropbox, Drive, or AWS because they keep a record of versions and do the back-up in the background, without you needing to do anything. Note that in some locations, those services may not be allowed because they are not specifically meant for sensitive data. The solution we use in our lab is a little bit more technically involved, but it is HIPAA and GDPR-compliant. You can read more about it in Gautheron et al. (2021), in the Resources section. 11.1 Resources Gautheron, L., Rochat, N., &amp; Cristia, A. (2021). Managing, storing, and sharing long-form recordings and their annotations. pdf "],["whereanalyze.html", "Chapter 12 Where should you run automated analyses 12.1 Resources", " Chapter 12 Where should you run automated analyses If you use LENA, most often you will have purchased one of the licenses that allows you to do your analyses on the cloud. This is great because it means that you don’t have to have a particularly powerful computer, and you’ll probably benefit from software and algorithm updates without even noticing. One downside, however, is that certain laws restrict storage and sharing of data across national boundaries, so if you are not based in the USA, you should consult local regulations to see if it is nonetheless possible for you to use LENA’s cloud-based service. Also within LENA, another option is to purchase their XP license. They do not recommend this because this license only works on machines running Windows 7 or 10, and it is not being updated. This means you either need to have a computer that always runs this operating system, or use a virtual machine system (see Resources for an explanation), which in our experience is not always easy to set up. If you are running open-source software, like the ACLEW options we discuss in another video, you have a choice between running your analyses in the cloud, in a cluster, or locally in your computer. If you’re not sure what these words mean, check out our resources for more information – but in a nutshell, when I talk about cloud-based services, I’m talking about Amazon Web Services, OVH, and other companies that have made their computing power available over the web. That means that in order to use them, you need to transfer your data to the cloud. This does not mean that you are sharing it or making it public. That transfer can be secure, and the storage can also be secure. So ask around in your institution before you decide against this option. The second word I used was “cluster” – in a nutshell, this word just means a group of computers hooked up together. Your institution or your country may have cluster services which you can consider. In this case also, you’ll have to make sure that the transfer and the storage are secure. For instance, you may want to ask whether anyone else will have access to the data while it is stored there. Regarding cloud-based services, these do require a bit of technical expertise to set up, and you do need to check local regulations, but we think it is very likely that if you are based in the Global North, you’ll be able to find a cloud-based service that has everything you need. We know less about the Global South, but if your local regulations allow transfer of data to Europe, you can probably use a European-based cloud service which should fulfill all regulations and ethical constraints. The Echolalia team is based in our same department, and are also working on this topic, so we asked them about this. They looked most thoroughly into using Amazon Web Services, or AWS for short. If you are in Europe, this is an option that is GDPR-compatible, and in the USA it is HIPAA-compatible. In other locations, please check local regulations, or get in touch to see if we have more information on this. If you cannot use cloud-based services, the next best alternative is to use a local cluster. Many institutions have these, so you can reach out to your local informatics department to ask. This option also requires a bit of technical background, so you probably want to collaborate with someone who knows about this to get the analysis pipeline set up. Once everything is installed, running the ACLEW analysis involves simply copy-pasting a series of commands, and so you just need minimal knowledge of using the Unix shell. If you don’t know what that is, there are short courses – in our experience, people who know another computer language (like python or R) learn bash basics in less than an hour. And if you don’t know other languages, it may take you 3-4 hours, so it’s still not very time consuming. We provide a link to a tutorial in the resources section. The big advantage of those two options is that you’ll be relying on powerful machines which will make processing faster. If neither of those work, you’ll be left with only one option, which is running these tools in your personal computer. You may think that this is a good option because then you don’t need to move your data around. However, as we explain on the video on storage and backup, it is a very bad idea to only have one copy of your data, as it may be accidentally destroyed or damaged. So you need several copies anyways. Running the software in your personal computer is not a great option for at least two reasons. First, you still need quite a bit of knowledge to get everything properly installed. We have had a myriad of problems trying to get this done, as each machine has a slightly different operating system and software already installed. Plus you still need some technical knowledge – not just using a shell, as in the case of the cluster (assuming that someone else will help you install things in the cluster). To get these tools installed in your machine will probably require general informatics knowledge, and lots of patience, as you look up errors and try out recommended solutions. The second disadvantage is that even when you’ve installed everything, the trouble is not over as it will take quite a bit of time to actually process the data. We haven’t thoroughly benchmarked ALICE but this table shows you how long it takes to process files using the voice type classifier. Batch size is the number of sequences your computer will process in parallel. This can decrease the running time but increase the memory consumption of the program. Personal computers typically have CPUs, not GPUs. In a nutshell, it’ll take about 4h to do a 16h recording in your personal computer. The precise numbers, however, will change depending on the specifications of your personal computer. batch_size GPU CPU 32 1/35 0.27 64 1/45 0.26 128 OOM* 0.25 256 OOM 0.24 Note: *OOM stands for out of memory. Of course, if you are in this situation, you can always try! We do know some people who were just patient and were able to use their macbook to run VTC on their 100+ files in about two weeks. After all, you will ideally only run this process once, so it may make sense to do this if the alternative of using cloud- or cluster-based computing will take you even longer to set up. 12.1 Resources LENA information on security What is a virtual machine Cloud versus cluster computing Unix shell tutorial Examples cloud services Examples clusters: France USA "],["evaluating.html", "Chapter 13 Evaluating your automated analyses 13.1 Basic concepts 13.2 Additional concepts 13.3 Resources", " Chapter 13 Evaluating your automated analyses Regardless of whether you are using LENA or the ACLEW tools, it’s always a good idea to check how accurate automated analyses are in your dataset. Here’s a demonstration of this: This graph shows the correlation between human and automated word counts for several corpora, all of which were collected from English-speaking families with infants between roughly 1 and 3 years of age. The correlations, however, vary quite a bit, particularly in the ACLEW tool called ALICE (yellow bars). We introduce ALICE in Video 5, and we discuss its accuracy overall in Video 14. But for now, what we want to say is general to any tool, and it has to do with the question of how you interpret results of automated analyses in general. To prove this to you, here’s another example, this time from LENA: the correlation between human counts of child-adult conversational turns and the corresponding LENA’s automated counts was .7 in one study on Vietnamese learners (Ganek et al., 2018), but only -.03 in another on Korean learning infants (Pae et al., 2016). So when you hear someone say that “LENA has been validated” – what can this mean? We think today what this means is that someone did a study to check how accurate LENA metrics were in a given sample, but it does not necessarily mean that the accuracy was high enough for the purposes you want to use LENA, or whatever other software. In this video, we want to introduce some concepts which will be useful to understand how to assess an algorithm’s accuracy, which is necessary for you to decide whether any algorithm is appropriate for your own research or applied goals. 13.1 Basic concepts We use the term accuracy to describe in general how well metrics extracted from a pipeline including some software is. So we will use this term a bit like an umbrella term. Accuracy can be assessed in many ways. As we saw in the Video 5, one of the first things algorithms do is say which stretches of the audio correspond to which speakers. Within speech technology, evaluation of this type of classification is done in two main ways: using recall, precision, and the derived F-score; and/or using false alarms, misses, and confusions, which can be combined in a diarization error rate, or an identification error rate. We define each in turn. We use the term “recall” to say: what proportion of acoustic stretches that a human calls a category, say “key child”, have been discovered by the machine and put in a given class, say “key child”. A little mnemonic trick: you can call recall “recatch”, to remember that it represents what proportion has been caught with a given name. It may be easiest to assume that the category is the same for now: so for instance, what proportion of the vocalizations that the human said were spoken by the key child were discovered and classified as key child vocalizations by the machine. In reality, however, the machine and the human could use different labels – perhaps the human calls the key child “Martin” and the machine “CHN”. But if you find that confusing, just focus on cases in which the machine and the human use the same label. To make this more precise, let us look at an example. This figure shows the recall for each one of the LENA categories. If you focus on the cell that crosses the LENA category CHN avec the human category CHI, you see 50%: 50% of the sections that the human identified as being spoken by the key child were detected with LENA’s ‘key child near’ category. Compare that with CXN-OCH: only 31% of the sections that the human identified as being spoken by other children were discovered by LENA’s ‘other child’ category. This means that LENA’s CHN category has a higher recall than their CXN category. Figure 13.1: Recall: Confusion matrix between LENA (x axis) and human annotations (y axis). In each cell, the top number indicates the percentage of all frames in that human category (row) that are labeled as a given class by LENA (column); cells in a given row add up to 100%. The number below indicates number of frames in that intersection of LENA and human classes. (reproduced from Cristia et al. 2020 BRM Precision represents instead what proportion of acoustic stretches that the machine calls a category, say “key child”, belong to a given category according to the human. This one is easier to remember – if someone asks you “but how precise are LENA’s key child labels?” it’s easy to answer by looking at this metric, and assuming that the machine and human labels are the same, because it just means what proportion of, for instance, bits the machine said were key child vocalizations truly were such according to a human annotator. In reality, just as with recall, precision can be calculated for every combination of labels across humans and machines. This figure shows the precision of each one of the LENA categories. If you focus on the cell that crosses the LENA category CHN avec the human category CHI, you see 60%: 60% of what LENA called ‘key child near’ was classified as ‘key child’ by the human. Compare that with CXN-OCH: only 27% of what LENA called ‘other child near’ was classified as ‘other child’ by the human. This means that LENA’s CHN category is more precise than their CXN category. Figure 13.2: Precision: Confusion matrix between LENA (x axis) and human annotations (y axis). In each cell, the top number indicates the percentage of all frames in that LENA category (column) that are labeled as a given class by the human (row); cells in a given column add up to 100%. The number below indicates number of frames in that intersection of LENA and human classes. (reproduced from Cristia et al. 2020 BRM Typically, classification algorithms are evaluated using metrics such as Recall, Precision, or F-score. Many metrics more specific to certain speech processing tasks have been developed, some of which can be easily be calculated in python using the pyannote-metrics package (Bredin 2017). Agreement coefficients commonly used in Psychology and Natural Language Processing (Krippendorff’ Alpha, Fleiss’ Kappa, etc.) can also be used to evaluate the reliability of automated annotations. All of these are available in the Child Project package, Introduced in 6. 13.2 Additional concepts We often hear that “LENA has been validated” – we think that what people mean by that is that there have been efforts to check its accuracy, but in some cases people do mean that its “validity” has been checked. In psychometrics, validity and reliability are actually technical concepts, which are often discussed together. In this context, the reliability of a measure captures the extent to which you can repeat that measure on the same individual and get a similar result. Precision and recall are informative of reliability, where the “repetition” comes from drawing the measure using human annotation versus automated annotation. So in a way, it is the “same” measure. Validity can be used contrastively to indicate that the measure of individual variation (variation across participants) has been checked against some other benchmark. For instance, the LENA Foundation did several studies to establish validity by testing the same children they collected long-form recordings from with a battery of speech and language tests. A recent meta-analysis suggests that there is a small-to-medium association (r=.27) between LENA measures and concurrent or longitudinal language tests (typically vocabulary), although 15 out of 17 included studies bore on English speakers (and 13 were carried out in the USA), so it would be worthwhile conducting similar research in other sites. It is important to bear in mind also that this relates LENA measures to child language, so if the long-form recording is intended to measure language experiences, then a different kind of validation would be needed. 13.3 Resources Ganek, H. V., &amp; Eriks-Brophy, A. (2018). A concise protocol for the validation of Language ENvironment Analysis (LENA) conversational turn counts in Vietnamese. Communication Disorders Quarterly, 39(2), 371–380. https://doi.org/10.1177/ 1525740117705094 Pae, S., Yoon, H., Seol, A., Gilkerson, J., Richards, J. A., Ma, L., &amp; Topping, K. (2016). Effects of feedback on parent–child language with infants and toddlers in Korea. First Language, 36(6), 549–569. https://doi.org/10.1177/0142723716649273 References "],["accuracy.html", "Chapter 14 Accuracy of automated analyses 14.1 LENA Software 14.2 ACLEW tools 14.3 Resources", " Chapter 14 Accuracy of automated analyses In this video, we are going to discuss accuracy of automated analyses. 14.1 LENA Software The LENA Foundation has made an important effort to document the accuracy of different aspects of their software. However, I admit I am more convinced by data produced by other researchers, who have a smaller conflict of interest: Even if researchers in the LENA Foundation have every good intention, it will be very hard for them to not prefer to see good results over poorer results. Additionally, I disagree with some analysis decisions they made in their initial accuracy reports: Most saliently, they only discuss confusion between children and adults, and thus do not report errors due to missed speech (i.e., low recall) nor confusion between the key child and other children, and across sexes among adults. For some research purposes and populations, the accuracy of other child and male adult categories are particularly important. In my view, at present, the most convincing evidence on LENA performance comes from a comparative analysis we did on a corpus that contained mostly North American children: Precision Recall Fscore CHI 60.91 50.07 54.96 OCH 27.34 29.97 28.59 FEM 63.87 31.96 42.60 MAL 43.57 32.50 37.23 As you can see, the precision is relatively high for the key child and female adult, lower for male adult, and really very low for other children, with just one quarter of sections attributed to other child truly being classified as such by human annotators. As for recall, LENA tends to miss a lot of speech, less so for the key child, but for all others, two thirds of speech found by human annotators was missed by LENA. As explained in the previous section, some research purposes do not depend so much on perfect precision or recall, but rather in the extent to which differences across clips or across children are reflected. In these cases, one can look at correlations between e.g. child vocalization counts according to LENA versus humans. The data on such correlations is much more encouraging: A systematic review found that child vocalization counts for LENA versus humans correlated above .7 for many samples (including non-English speakers), with similarly good results for adult word counts. Results for conversational turns were much worse however, and work published since has raised concerns about this measure. 14.2 ACLEW tools 14.2.1 Voice type classifier First let’s start with the voice type classifier. We checked performance in a few ways. Let me start with results in a completely held out set: the algorithm had never seen data from those corpora. We used data from the English portion of ACLEW, which matches the language on which LENA was trained – but not our Voice Type Classifier, which was trained with multiple languages. The fact that LENA got to see more data in this language could lead LENA to have better performance than the Voice Type Classifier. Precision Recall Fscore CHI 62.37 76.67 68.78 OCH 46.77 25.78 33.24 FEM 70.30 57.87 63.48 MAL 39.52 46.92 42.91 Here is the performance for each talker type – in machine-learning terms, we call these “classes”. KCHI is the class corresponding to the key child, chi that corresponding to other children, fem to adult females, mal to adult males. You can ignore speech (a class that is activated whenever anyone talks), and AVE (the average). The first two columns show precision and recall. Precision means: when the algorithm said that something was a given talker what proportion of the time it actually was that talker? Recall means: what proportion of sounds or segments that were originally described as being a given speaker by human annotators were actually classified into that class by the algorithm? You can combine both of these types of information together into a measure that’s called f-score – shown here in the last column. In all three measures, higher is better. For recall, higher performance means that the algorithm successfully identified more of that class. For precision, higher precision means that more of the audio identified as a given class truly WAS that class. And finally, higher F-score can be obtained either because the algorithm found more of what it was looking for (so higher recall) and/or because it was more accurate (higher precision). You can see here that in this test set (only English learning infants, recorded with a LENA device) we find pretty good scores for the key child and female adult, lower for male adult, and lower for other children. How does the LENA software fare on the same data? Here is the F-score in a bar plot, so it’s easier to compare them side by side. We find better fscores for the voice type classifier than in LENA for all the classes. The increase in performance is particularly salient for key child and female adult, which also tend to be the most common classes. This is great news because it means overall better performance. That is the performance in a completely held out set, which is great because the algorithm had never seen data from those corpora and it allowed us to benchmark against LENA. But you may be interested in how good performance is across languages. For this, we are going to show you results from our “test set” – this is a set of data that was drawn from the same corpora as the training set. But note that the test set is “new”, since we didn’t use it for training. That said, generalization here is smaller in terms of the fact that these data come from the same corpora that were used for training. One advantage of this evaluation is that it covers more languages – remember that the corpora we used included English, French, a Chinese language, and many more. Moreover, the data were collected with a wide array of recording devices: LENA devices weren’t even the majority of the data. In this case, we see here that performance is also quite good for KCHI (the key child) and FEM (female adult), lower for MAL (male adults), and relatively low for other children (CHI). Precision Recall Fscore CHI 81.69 73.48 77.37 OCH 18.78 40.45 25.65 FEM 77.94 87.40 82.40 MAL 37.82 47.86 42.25 However, please note that, as in the LENA case, performance for male speech and other children’s speech is not very good – so if you are interested in those classes, there is currently no algorithm that is perfect for them. That said, one of the goals of the exelang project is to improve performance on those classes so – stay tuned! 14.2.2 ALICE Let us now discuss performance of our word counting open source alternative, ALICE. In this chart we’re showing you the evaluation for ALICE: the y axis shows correlations between the word count estimation according to an algorithm and the human – higher is better because it means the two agree more. Each one of the bars shows a different estimation, using different algorithms. We are going to focus only on two of these bars: The bars for ALICE’s word count estimation are yellow whereas those for LENA are purple. We didn’t have LENA performance for all of the corpora because some of them were gathered with an olympus device, and as explained in the hardware video, you cannot annotate data with LENA unless you use a LENA device. In this evaluation, we only looked at languages that were represented in the training set, and also we retrained each language separately. The corpora used contained mostly English, with some Spanish, Tseltal, and a little bit of French. In terms of recording devices, it was mostly LENA, with some data collected using olympus and other similar devices. Notice that in general performance is quite good, and certainly comparable to LENA: it’s much better than LENA in two cases (the bergelson corpus, which is English-spoken and the LUCID 0-5 corpus, which is also English spoken), LENA is quite a bit better in one case (the McDivitt corpus, which is mainly English spoken) and just a little better in the last case having results for both (the Warlaumont corpus, mostly English). As for differences across languages and devices, it does not appear to be the case that the bars are always higher (ie correlations are always better) for English spoken corpora collected using LENA (BER, MCD, WAR, L05) than for corpora collected in other languages and with non-LENA devices (Tseltal, the Rosemberg corpus of Argentinean Spanish, and Yélî Dnye). Members of the ACLEW project also developed software to classify adult speech into child- versus adult-directed, but we feel performance of that algorithm is still not good enough to make a public release. We also developed a system for classifying the key child’s sections into different vocalization types (crying, canonical, and non-canonical). This software is open source and available via a virtual machine, which we found in the past is a little hard to install and use. So for these two things, please stay tuned – there will be developments in the future! 14.3 Resources LENA reliability original report Räsänen, O., Seshadri, S., Lavechin, M., Cristia, A., &amp; Casillas, M. (2020). ALICE: An open-source tool for automatic measurement of phoneme, syllable, and word counts from child-centered daylong recordings. Behavior Research Methods. pdf code Lavechin, M., Bousbib, R., Bredin, H., Dupoux, E., &amp; Cristia, A. (2020). An open-source voice type classifier for child-centered daylong recordings. Interspeech. pdf code "],["humanannotation.html", "Chapter 15 Human annotation 15.1 Checking accuracy 15.2 Doing things automated analyses could do but don’t do quite well yet 15.3 Getting complementary information 15.4 Overarching questions: Deciding which sections to annotate 15.5 How should the clips be selected? 15.6 What should be the length of each clip? 15.7 Softwares for human annotation 15.8 Resources", " Chapter 15 Human annotation Automated analyses are great because they can capture patterns over all of the data for each child, but you may consider also doing some annotations by humans. We see three purposes for human annotation: To check accuracy of automated analyses To do things that automated analyses don’t yet do To complement with more qualitative data We provide some pointers for each of the three goals. 15.1 Checking accuracy If you are working with a population for which automated tools’ accuracy has not been checked against a human standard, it is a good idea to do some annotation to this end. We provide you with information about how to do this here. Note that if you are working with English-speaking or bilingual children in North America, there have been several validation studies, so it is probably not a good use of your resources to do yet another validation study. Before you start, we recommend a literature search starting from Cristia, Bulgarelli, &amp; Bergelson (2020)’s systematic review (in this video’s References), to make sure that there really is no reliability data for a similar sample. If no reliability studies exist, then you can use the data you have collected in a pilot or your study. Do not collect data in a different environment from the one you are aiming to use the recorder in. In particular, do not record yourself reading a book next to the recording device in a sound-proof booth. The accuracy in such a setting will tell you very little about the accuracy of the algorithms in your spontaneous, naturalistic, child-centered sample. 15.1.1 How to annotate and run the evaluation Imagine that you decided to draw 10 clips x 2 minutes randomly from each of 10 children. This is about 3h 20min of data, which takes roughly 90h to annotate, in our experience. We recommend training annotators using the ACLEW Annotation Scheme, which has an online test annotators can go through to ensure accuracy of their own annotations. Once the manual annotations are complete, the machine annotations can be extracted and compared against the human annotations easily, provided you are using ChildProject to organize your data. We have a separate video where we introduce you to ChildProject 6. In a nutshell, this will allow you to extract key classification accuracy measures used here (false alarm rate, miss rate, confusion rate and the derived identification error rate), as well as CVC, CTC, and AWC comparing LENA® and human annotations. We explain all of these terms in the video about Reliability and Validity. We insist that re-using our code is only possible “off the shelf” for manual annotations made using the ACLEW Annotation Scheme for the annotations, and ChildProject for organizing your data, although if you know how to program you can also adapt it to other schemata. 15.2 Doing things automated analyses could do but don’t do quite well yet You may be interested in quantifying speech addressed to the key child versus to others, to separate child-directed from overheard speech; or perhaps you want to estimate the complexity of the child’s vocalizations in terms of the sounds the child produces. Both of these goals are almost within reach of automated analyses. It is likely that both can be done with fairly local information – that is, by listening to a sentence, you may be able to tell who it is spoken to. You may not need to know the whole context of the conversation. If that describes the kind of information you are hoping to extract, we strongly recommend considering to rely on citizen scientists – see the Video on 8 for information on seeking IRB approval for this, and Zooniverse for an excellent citizen science platform. There are ways of processing your data so that it can be hosted in such a platform without revealing participants’ identity or personal information. In some cases, you do need a little context – at the very least to decide whether a child vocalization is meaningful or not. At this point, this is not a task that can be solved by a machine, and in fact it takes quite some training for humans to do it reliably and replicably. If this is the kind of thing you were thinking of, we recommend looking at Mendoza and Fausey’s and Soderstrom et al’s papers in the Resources section for more ideas. 15.3 Getting complementary information You may want to get ideas of the warmth of the interactions, or the contexts in which different languages are used in a multilingual household. In this case, we recommend the work of Cychosz, Villanueva, &amp; Weisleder for more ideas. 15.4 Overarching questions: Deciding which sections to annotate Human annotations require to define a sampling process, i.e. an algorithm to decide which portions of audio should be annotated. This raises a few questions, among which: How much audio should be annotated (in total)? How should the clips be selected? What should be the length of each clip? Of course, the answer to each of this question depends on your research goals, but we provide here some considerations to help you decide. 15.4.1 How much audio should be annotated (in total)? Most often, the total amount of audio that can be annotated is heavily constrained by the availability of human resources, and can be considered fixed. For instance, you have a budget for a research assistant, or limited time because you need to turn in your thesis. Even in these cases, consider our arguments here, because they will let you think about the kind of phenomenon you can look at. The quality of estimations based off partial annotations primarily depends on the volume of audio that have been annotated. There are several reasons to use partial annotations to extrapolate metrics over the whole data: No automated tools exist or are accurate enough for the task that is being considered Automated tools exist, but due to limited resources and/or time, they cannot be run over whole recordings. To illustrate the effect of subsampling on the quality of estimates derived from annotations, we used annotations generated by the Voice Type Classifier over recordings of the Bergelson dataset (Bergelson 2015). We compared the estimates based from subsamples with various annotation budgets to the values obtained for the whole recordings. Figure 15.1: Relative error between speech duration estimates based on subsamples and the values for the whole recordings, using the Voice Type Classifier. (periodic sampling; 1-minute clips.) The level of accuracy to achieve, and therefore the volume of audio to annotate, depend on the size of the effects being studied. 15.5 How should the clips be selected? 15.5.1 Unbiased sampling For many tasks, the clips should be selected in a way that does not induce biases in the results. For instance, if one plans to estimate the quantity of speech produced or heard by the child, then it is important to use unbiased estimators. In order to evaluate the performance of automated tools, it might also be preferable to use unbiased annotations which can assess the behavior of the algorithms in a variety of contexts rather than for a subset of configurations. We can cite at least two ways of achieving such an unbiased sampling: By choosing each clip at random, independently from past choices By sampling the audio periodically (so that the position of the first clip completely determines the positions of subsequent clips) Q: shouldn’t be better to sample regions where we know the child to be talking a lot, or where there are lots of conversational exchanges between adults and the key child? A similar question is whether you cannot use annotations that you have sampled like that, for qualitative analyses, to have an idea of accuracy. If you have a limited annotation budget, of course re-using your annotations this way is better than nothing at all, but do note that if you sample your recording non-randomly, it is less certain that your conclusions based on such clips will generalize to the whole of the recording. For instance, imagine that you were trying to decide whether the child talks little or a lot based on sections that have been selected because the child talks – that would make it difficult to know because you’ve purposefully selected regions where they are talking, giving you a sort of ceiling effect. Q: When I sample randomly or periodically, I don’t have a lot of category X - is that a problem? If there are no samples of a given category, then accuracy of that category cannot be evaluated; and if there are only a few, then it is possible that these are special in some way and accuracy estimates may not generalize well to others. So if the sections you end up with have no “other child” or “male” speech, then perhaps you’ll be uncertain of how well the algorithm picks up these voices. 15.5.2 Targeted samplers Although unbiased samplers are necessary for many purposes, targeted samplers may be useful in a number of cases. One such case is the training of a computational model that requires a great amount of manual annotations. Usually, annotations obtained from portions of audio that contain little to no speech are uninformative for these models. Therefore, it may be much more efficient to let humans annotate portions of the recordings that have a higher chance of containing speech. For this purpose, one could at least conceive two strategies: Using a Voice Activation Detection model or more sophisticated models (e.g. linguistic unit estimation, speaker diarization, etc.) Choosing portions of the audio that have a strong signal energy Using sophisticated models may yield a more efficient pre-selection. However, it is harder to predict the effects of their own biases, and a cruder selection method (such as the energy detection) may still show good results while being less arbitrary. Below, using 2 hours and 40 minutes of human annotations from the Solomon Islands corpus, we show how selecting portions of audio based on the signal energy alone can increase the amount of speech. This benchmark is based on the Energy Detection Sampler of the ChildProject package. We used a 50 Hz - 3000 Hz passband in order to increase the signal/noise ratio. Figure 15.2: Fraction of 30 second windows above some energy quantile that contain speech from a given speaker class. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult. Figure 15.3: Average speech time for each speaker class depending on the energy quantile. CHI = key child, OCH = other children, FEM = female adult, MAL = male adult. This example shows that the energy-based sampler clearly suppresses silence. However, it also induces a bias by favoring some classes (CHI and FEM) over others (MAL and OCH). One explanation is that the first two (key child and female adult) are closer to the device (especially the key child, by design) and thus sound louder on average. 15.6 What should be the length of each clip? The answer, again, will depend on the research question. For instance, exploring conversational behaviors may require longer clips than only assessing the amount of speech. However, here we show a way in which the choice of the sampling strategy may have a significant impact. We used automated annotations derived from 500 recordings from the Bergelson dataset (Bergelson 2015). Automated annotations are easy to produce in large amounts; this allows us to simulate a sampling method using semi-realistic distributions of speech. For a constant annotation budget of 20 minutes per recording, we estimated the total amount of speech for each recording using only annotations extracted with two sampling strategies - random clips vs periodic (i.e. evenly spaced) clips - and different lengths of clips. This allowed us to estimate the relative difference between the quantities of speech estimated from 20 minutes of annotations and the true quantities. The results are shown below. Figure 15.4: Performance of several sampling strategies. (500 recordings; constant annotation budget of 20 minutes per recording.) It can be seen that the estimations are much more accurate when using many short clips rather than a few longer clips. The periodic sampling also yields more accurate results than random sampling. This result is rather intuitive: annotating a portion of audio in close temporal proximity with a previous portion will yield much less information, because the two portions are correlated. Using annotations that are farther from each other helps probe a wider variety of configurations and thus yields better speech quantity estimates. This does not mean, of course, that one should annotate extremely short clips. First, in practice, two 1-minute clip will take longer to annotate than one 2-minute clips, especially if annotators needs to listen to the context to make better decisions. Reducing the length of the clips may thus rapidly decrease the efficiency of the annotation process. Moreover, some estimators, such as vocalization counts, may be strongly biased by using short clips, due to the increased significance of boundary effects. Also, there is one exception to this general recommendation of prioritizing shorter over longer clips: Often, colleagues use Pearson correlation coefficients between the metrics derived from the algorithm (e.g. vocalization counts or speech duration) and the human coding of the same sections to look at reliability. We talked about the fact that there are many ways to quantify the quality of automated annotations from a gold standard established by expert annotators in 13. We looked at whether this measure was affected by the choice of the clip duration using the Bergelson dataset (Bergelson 2015). As above, we used LENA and VTC as our two algorithms (having human annotation would have been great, but we don’t have that for the whole 500 recordings). We then extracted clips of e.g. 1 minute of duration from both the VTC and LENA annotations and calculated the correlations across e.g. the child vocalization counts by the two algorithms. We repeated for other speech metrics (both vocalization counts and speech duration for both children and adults); and we did the same for clips of other lengths. Results are shown below. Figure 15.5: Pearson correlation between LENA and VTC derived metrics as a function of clips length. (500 recordings; periodic sampling; constant annotation budget of 60 minutes per recording.) We can observe a clear increase in the correlation coefficients as the length of the clips increases. This means Pearson correlation coefficients for these metrics are not robust to the length of the clips used to evaluate the algorithms, and actually benefit from longer clips. But we know longer clips have issues: they can have greater estimation errors (as we saw above). In contrast, Recall and Precision remain stable for varying clip lengths, as shown below: Figure 15.6: VTC vs LENA key child recall/precision as a function of clips length. (500 recordings; periodic sampling; constant annotation budget of 60 minutes per recording.) Our conclusion is that perhaps the field should turn to these other metrics, but in the meantime, you should be really careful when thinking about why you are doing annotation, what you are going to use them for, and how they may be suboptimal for other goals. 15.7 Softwares for human annotation Software Key strenght System Input Multitier Closed vocab Free text Spectogram Large files Interoperability Modes Praat (Boersm a, 2009) ideal for acoustic phonetics support A timed no yes yes limited CLAN, Phon both [Phon] (https://www.phon.ca/phon-manual/getting_started.html) (Rose et al. 2007) ideal for phonological level support, OS AV both yes yes no yes Praat, CLAN both Transcri berAG ‘recommended’ by LENA OS A timed no yes yes yes none easy [Datavyu] (http://www.datavyu.org/) (Datavyu Team, 2014) User-defined key strokes support, OS AV+ untimed yes no no no none both [ELAN] (https://tla.mpi.nl/tools/tla-tools/elan/) (Sloetjes &amp; Wittenbu rg, 2008) Multi-stream, use of template, interopera ble support, OS AV both yes yes no yes CLAN, Praat Transcriber, AG, … both [CLAN] (http://childes.psy.cmu.edu/) (MacWhi nney, 2000) Ideal for lexicon and grammar support AV untimed no yes no yes Praat, Phon, ELAN both 15.8 Resources Cristia, A., Lavechin, M., Scaff, C., Soderstrom, M., Rowland, C., Räsänen, O., Bunce, J., &amp; Bergelson, E. (2020) A thorough evaluation of the Language Environment Analysis (LENA) system. Behavior Research Methods. preprint online resource Cristia, A., Bulgarelli, F., &amp; Bergelson, E. (2020). Accuracy of the Language Environment Analysis (LENATM) system segmentation and metrics: A systematic review. Journal of Speech, Language, and Hearing Research. online resources, including pdf Cychosz, M., Villanueva, A., &amp; Weisleder, A. (2020). Efficient estimation of children’s language exposure in two bilingual communities. pdf Mendoza, J. K., &amp; Fausey, C. M. (2021). Quantifying everyday ecologies: Principles for manual annotation of many hours of infants’ lives. pdf Soderstrom, M., Casillas, M., Bergelson, E., Rosemberg, C., Alam, F., Warlaumont, A. S., &amp; Bunce, J. (2021). Developing A Cross-Cultural Annotation System and MetaCorpus for Studying Infants’ Real World Language Experience. Collabra: Psychology, 7(1), 23445. pdf Casillas, M. &amp; Cristia, A. (2019). A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings, Collabra. See Appendix F: Annotation software overview \" References "],["secondaryanalyses.html", "Chapter 16 Secondary analyses 16.1 Resources", " Chapter 16 Secondary analyses If you have opted into our ChildProject framework, then there are a number of secondary analyses that become incredibly easy. We have thought about the most typical routines you need to engage in, and provide sample code or direct functions for all of them. In particular, one thing you typically want to do is estimate the child-session-level metrics. For instance, one child was recorded for 8 hours one day and 16 hours another day. You don’t want to have the play-by-play, that is, when the child was vocalizing. But instead, you may be interested in finding out, in total, how much they vocalized in each of these recordings. Since the recordings lasted a different number of hours, you may want to get the recording length and estimate a number of vocalizations per hour instead. Getting all of these numbers is easy once your data are in the ChildProject format, and you can find example code for estimating these “metrics”. Another frequently encountered issue is how accurate automated analyses are. We discussed in the Video on 15 that to this end, you may want to do human annotation. You will find an example of how to set up an annotation campaign in the Resources. Moreover, once you’ve gotten some sections annotated, how do you estimate the reliability? ChildProject functions make it easy to calculate recall, precision, agreement, and other reliability metrics (such as error rates and correlations). Check the Resources section for an example on reliability. Some of you have so much data that you may want the help of citizen scientists in the Zooniverse platform to get them annotated. We have an example project that shows you how to extract clips and cut them up into short sections, so they can be shared over Zooniverse, and then get back the annotations, analyze them, and integrate them with your dataset – this is the best practices for Zooniverse example. Do you have other questions for analyses not covered above? Well, peruse the “show and tell” conversations in the ChildProject discussion board for some ideas, and feel free to create your own! 16.1 Resources Metrics: code to extract, example analysis Reliability: how to example Zooniverse annotation campaign example "],["results.html", "Chapter 17 Writing up results 17.1 Scientific venue 17.2 Families 17.3 Communities 17.4 Resources", " Chapter 17 Writing up results In this video, we are going to give you some tips and ideas for how to write up your results for three types of readers. First, we will discuss write-ups for a scientific venue. Second, we will discuss the kind of information on your results that you can share with participating families. Third, we cover write-up of results for the benefit of the community that you sampled from. 17.1 Scientific venue Regarding your write-up to a scientific venue, we are mostly going to give you key information and references for the methods section. First, in the participants section, mention not only participants and data that you are including but also all the data that was collected but excluded. Two potential reasons for exclusion that are relevant in the participants section are short and silent recordings, because they suggest the family “opted out”. For instance, if all families recorded for 16h but one did so for 2h, then this may suggest they changed their mind about participating. Alternatively, recordings that have mostly silence (e.g. over 90%) may mean that the family put the recorder in a drawer, because they did not want to be recorded. Second, in the equipment section, mention not only what device you were using but also clothing, and how the device was attached to the clothing. You need to convey that the device was attached in a tight-fitting way, to minimize noise. Next comes the analysis section. Here, you can mention the software you used to analyse the recordings automatically, and provide links to more technical reports. Links to the technical reports to the all systems are provided in the resources of this video. Make sure you also mention relevant studies on reliability. We have a dedicated video on this, but in a nutshell, if you are interested in broad measures, like child vocalization counts or adult word counts, then there is a meta-analysis on the accuracy of these metrics, and another that measures correlations between these measures and standardized language tests. However, please note that these references do not cover all useful metrics. For instance, if you are interested in overall vocalization counts by the key child (including speech and crying), neither of those meta-analyses can inform you on that. However, looking for studies that cite those meta-analyses will probably help you discover any newer evaluations. If you have carried out any human or manual annotations, make sure that you provide the link to the human annotation training material. Instructions you have provided annotators will affect your data, so they should be documented in the same way as you report the make and model of your recording device. Make sure you also report on any inter-rater reliability data – where raters may be two humans, or between a human annotator and the algorithm. 17.2 Families In my lab, we provide summaries with the results of the studies to the families who participated in the study, so we thought to give you some tips on how to explain results to families. Some parents want to get their own data, so you can produce an individualized graph which shows the proportion of silence, vocalizations, etc. found in the recorder. As mentioned in the piloting video, we do not think giving families the raw data is a good idea on ethical and legal grounds, but providing a visual snapshot of the recording may be even more interesting to the parents than 10+ hours of audio! Some parents want more, for example a comparison between their quantity of speech and the other families in the study. It is up to you (and the community you are working with) to you to decide whether it is reasonable to give them this information. In my lab we reason as follows: This method is still being developed, and we don’t have a very large, representative, and normed sample on which to base our measures. Therefore, providing parents with percentiles might generate some unneeded anxiety on parents. For example, if you have 30 children in your sample, someone is going to be the 30th, and this family may perceive these results as discouraging. Another thing you might want to do is provide them with group results, which might be a summary of your scientific paper. You can also provide them with the link to the actual paper. This is important so that they have access to the publication that enters the scientific stream. However, we are uncertain that just giving them the paper will be useful, as it will typically contain technical language, and may be pitched for specialists. So a separate write-up is appropriate, and much faster than a publication. In such a write-up, follow the usual advice for science communication: avoid technical wording, instead using a language that is understandable by someone who doesn’t have the technical background, and cover aspects of your data that families are actually likely to care about, which you may have discovered during piloting. 17.3 Communities Long-form recordings are capturing people’s lives to a certain extent. In fact, if you think about it, they are not just capturing the participating families’ lives, but they are in fact sampling from the lived experiences of the communities those families belong to. So in a way, long-form recordings are taking a snapshot of the life in people in a given community. Communities don’t necessarily have a legal identity but they might be seen as a superset of all of the families that participated in your study. So, if this sounds something that applies to the work you are doing, you can think of ways to give back to the community and not just the specific families who participated. You can do this by for instance creating a summary of the results or short presentations of your study and what you found so far, perhaps the same one you are giving participating families. You can then distribute it to associations or other entities related to that community. Even better, you can collaborate with associations closely connected to your community in producing such a write-up. Or you could try to write a newspaper article, if you know a journalist who can help you with that. In any case, this is a great opportunity to go back to the topics that were raised as key interests by members of the community during the piloting stage. 17.4 Resources Example scientific paper not using LENA: Casillas, Marisa, Penelope Brown, and Stephen C. Levinson. “Early language experience in a Tseltal Mayan village.” Child Development 91.5 (2020): 1819-1835. pdf Example scientific paper using LENA: Ma, Y., Jonsson, L., Feng, T., Weisberg, T., Shao, T., Yao, Z., … &amp; Rozelle, S. (2021). Variations in the Home Language Environment and Early Language Development in Rural China. International Journal of Environmental Research and Public Health, 18(5), 2671.link Example scientific paper using LENA looking at methodology: McDonald, M., Kwon, T., Kim, H., Lee, Y., &amp; Ko, E. S. (2021). Evaluating the Language ENvironment Analysis System for Korean. Journal of Speech, Language, and Hearing Research, 64(3), 792-808. link Example information for parents (French) LENA reliability original report LENA Meta-analysis on accuracy: Cristia, A., Bulgarelli, F., &amp; Bergelson, E. (2020). Accuracy of the language environment analysis system segmentation and metrics: A systematic review. Journal of Speech, Language, and Hearing Research, 63(4), 1093-1105. pdf LENA Meta-analysis on metrics and standardized tests: Wang, Y., Williams, R., Dilley, L., &amp; Houston, D. M. (2020). A meta-analysis of the predictability of LENA™ automated measures for child language development. Developmental Review, 57, 100921. link VTC Technical report: Lavechin, M., Bousbib, R., Bredin, H., Dupoux, E., &amp; Cristia, A. (2020). An open-source voice type classifier for child-centered daylong recordings. Interspeech. pdf code ALICE Technical report: Räsänen, O., Seshadri, S., Lavechin, M., Cristia, A., &amp; Casillas, M. (2020). ALICE: An open-source tool for automatic measurement of phoneme, syllable, and word counts from child-centered daylong recordings. Behavior Research Methods. pdf code "],["datadonation.html", "Chapter 18 Data donation / Transfer 18.1 You don’t have data yet 18.2 You already have data 18.3 Resources", " Chapter 18 Data donation / Transfer If you are considering sharing your data with ExELang, you are probably in one of the following situations: You already have collected data and you want to share it with us; or You don’t have data yet but you are willing to collect it and share it with us. 18.1 You don’t have data yet Let’s start with the easier option: you haven’t collected data yet. In this case, we’ll typically need to sign a bilateral agreement. This is ideal because we are going to agree on several things: you’ll always keep ownership (and main responsibility) over the data; what you get from us can include expertise hours, equipment, data processing, and funding for logistics; what we would like to get is the possibility to re-analyze your data within the scope of ExELang. Please note that in the agreement, we’ll agree on an appropriate scope for intellectual property. At ExELang, we are specifically interested in relating children’s experiences with how they talk, so we will ask for the ability to analyze and publish about this one point. If that covers your own research goals, we will discuss how to frame intellectual property so that it is ideal for both of us. If this sounds complicated, you can always just start collecting data on your own, and eventually get back to us when you have your data – in which case, you can watch the video “you already have data”. 18.2 You already have data If you already have data, there are four different options: 18.2.1 You have already shared your data in a scientific repository Great, we only need to access you data! 18.2.2 You haven’t shared your data in a scientific repository yet but you are willing to do so This is our preferred option! Sharing data in a scientific repository might be good for you and your study because it allows you to get many potential collaborators and citations by making the effort of uploading your data only once. Moreover, you are contributing to make science an open source for everybody. If you are not familiar with scientific repositories online, we can give you suggestions based on our experience with each one of the repositories, so to make it easier for you to decide which one works best for you. Feel free to comment if you have other questions for us! Repository name Project file Formatting How to update data Data access by non-curators Other features [Open Science Framework] (https://osf.io) No specific requirements for files or project structure Via browser A choice of: None (completely private), Invited people can read (and write), Anyone can read Plugins for software such as Google Drive, GitHub (GitHub, 2018), and others; Storage in USA or Europe [Databrary] (https://nyu.databrary.org) Some aspects of project structure specified Via browser A choice of: None (completely private), Invited people can read (and write), Anyone can read Data annotation with Datavyu software (Datavyu Team, 2014), some APIs [HomeBank] (https://homebank.talkbank.org) Project structure and file structure must follow one specific format Through personal contact with HomeBank personnel A choice of: None (completely private); Any HomeBank member can read; Selected HomeBank members can read; Anyone can read Data annotation and analyses with CLAN software [The Language Archive] (https://archive.mpi.nl/tla/) Some aspects of project structure specified Via browser A choice of: None (completely private), Invited people can read (and write), Anyone can read Data annotation with ELAN software 18.2.3 You don’t want to upload your data on a scientific repository but you are willing to create a license A License is an official permission granted by the owner of some Work (the “Licensor”) to other people (the “Licensee”) and governing how the Licensee is allowed to use the Licensor’s Work. This option allows you to make the effort of establishing rules for potential collaborators/citations once, and then re-use this license with others. When you create your license, you make the rules, so you can specify whatever you feel is best. See the resources section for some examples. You can also use open source licences, but you need to know that they allow for re-distribution (i.e., the people who get hands on your corpus can re-distribute it). Just in case, we made a list of the most commonly used ones: GNU: The GNU General Public Licence (GPL) is probably one of the most commonly used licenses for open-source projects. The GPL grants and guarantees a wide range of rights to developers who work on open-source projects. Basically, it allows users to legally copy, distribute and modify software. It’s a copyleft license: it means that the derived works need to have the same type of license. MIT: The MIT License is the least restrictive license out there. It basically says that anyone can do whatever they want with the licensed material, as long as it’s accompanied by the license. APACHE: The Apache License, Version 2.0, grants a number of rights to users. These rights can be applied to both copyrights and patents. 1) Rights are perpetual, 2) Rights are worldwide, 3) Rights are granted for no fee or royalty, 4) Rights are non-exclusive, 5) Rights are irrevocable, 6) Redistributing code also has special requirements, mostly pertaining to giving proper credit to those who have worked on the code and to maintaining the same license. It’s a non copyleft license: it means that the works derived can have a different type of license. CREATIVE COMMONS: Creative Commons (CC) licenses aren’t quite open-source licenses, but they are commonly used for design projects. A wide variety of CC licenses is available, each granting certain rights. A CC license has four basic parts, which can be enacted individually or in combination. Attribution. The author must be attributed as the creator of the work. Beyond that, the work can be modified, distributed, copied and otherwise used. Share Alike. The work can be modified, distributed and so forth, but only under the same CC license. Non-Commercial. The work can be modified, distributed and so on, but not for commercial purposes. No Derivative Works. This means you can copy and distribute the licensed work, but you can’t modify it in any way or create work based on the original. These parts of the CC license terms can be combined. The most restrictive license would be the “Attribution, Non-Commercial, No Derivatives” license, which means that you can freely share the work, but not change it or charge for it, and you must attribute it to the creator. This is a good license to get your work out there but still maintain more or less complete control over how it is used. The least restrictive would be the “Attribution” license, which means that as long as people credit you, they can do whatever they like with the work. 18.2.4 None of the above mentioned options works well for you In this case, we are back to the bilateral agreement. This is ideal if we want to agree on something very specific (e.g., we want to make an exchange – for instance, we fund your re-consenting the families who participated, in exchange for the possibility of re-using the data). Please note that in the agreement, you’ll always keep ownership over the data; and that we’ll agree on an appropriate scope for intellectual property. At ExELang, we are specifically interested in relating children’s experiences with how they talk, so we will ask for the ability to analyze and publish about this one point. If that covers your own research goals, we will discuss how to frame intellectual property so that it is ideal for both. See the resources for an example of a Data Transfer Agreement. 18.3 Resources Examples of licenses https://www.ldc.upenn.edu/data-management/using/licensing License for BabyTrain Corpus Example of a Data Transfer agreement Casillas, M. &amp; Cristia, A. (2019). A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings, Collabra. See Appendix B: Further information on archiving \" "],["field.html", "Chapter 19 Tips for field workers 19.1 Devices 19.2 Clothing 19.3 Logistics 19.4 Storage 19.5 Resources", " Chapter 19 Tips for field workers This video will focus on how to adapt data collection specifically to field conditions. First of all, by field conditions we mean cases in which you work far away from your usual lab, sometimes you won’t have a lot of access to water or to electricity (or both of them), and you don’t have a lot of technical support. The water is important when you need to wash your t-shirts, or any other kind of clothing you want to use for the hardware while electricity is fundamental for sure to recharge the devices used for data collection but also if you have some kind of in-the-field data annotation, this is also something to bear in mind. 19.1 Devices In field sites, we recommend USB devices. This is because they are very cheap and don’t look very fancy, so the chances you get them stolen are very low. But most importantly, they don’t require much energy. You can charge them with a simple solar panel, for instance. USBs are optimal if you are going to work in a place where you and your team are the only ones around who use a computer, because, as we mentioned in the hardware video, USBs don’t have any encryption, and they have a low bar for people to steal or copy the data from them if they come across one of these devices. You may still be thinking of LENA. Another issue we have come across with using LENA in the field is that it requires quite a bit of energy to be recharged, even if it is recharged from a USB port. Moreover, transferring the data is not that fast, and you need to do something special in your computer to extract the recording, which will not be immediately available to you. That is, even with a PRO License (which allows you to use the software in a portable computer), you don’t extract the recording directly, but instead, first you need to extract the data, then analyze it (which takes several hours of processing), and only then you can export the audio data. In our experience, we do not take all of these steps because the field sites we work in are not on the grid. Instead, we only take the step of transferring data from the device to the computer, and leave the processing and export for when we are back on the grid. 19.2 Clothing Regarding clothing, using paper or money clips to attach the device to the child’s clothing is probably your top option. That said, if the regular clothing is loose, then we don’t recommend using a money clip. As explained in the Clothing video 3, this will result in relatively poor data quality. Before you make your final clothing decision, we encourage you to talk to members of the community and get their opinion. In our experience, communities have very strong ideas about how they feel about clothing. Some communities really want the clothing that the child is wearing to alert everyone that the child is being recorded, so they will be happy with you having the child wear a funny-looking t-shirt or vests and they would choose that over alternatives. This way, if you are working in a village, everybody in that village would know that they could be recorded if they come close to the child, and can easily avoid it. It may also happen that everybody in the village already knows that the child is going to be recorded, so there is no point in having funny-looking vests. They might actually feel that if the child was wearing something funny-looking, the other kids would mock him/her, so they don’t want that for their child. In short, clothing is something you really want to talk to the communities about and have their feedback about it. When we are working with similar communities, we typically decide to have one t-shirt made for each family, so something that we might offer to the families that participate is to keep the t-shirt. In fact, when you make the t-shirts, they are often not so easy to wash but quite cheap to make. So, for instance, if you use a regular t-shirt you might spend maybe 4 US$, and use a paper clip to attach the device to the t-shirt. This way, the family can keep the t-shirt and it only costs 4 US$ for you. In contrast, if you buy LENA t-shirts, each one costs 25 US$, so you may want to buy fewer and wash them. This also adds to the work you need to do for each family that is recorded, of course. 19.3 Logistics The other thing to bear in mind is how you are going to get the devices to and from different places. If you are using LENA, typically you will have set the time to the local time, so that’s fantastic because you will have the times of the recordings. If you are not using Lena, and you are using an USB or Olympus instead, often you won’t have a correct time stamp for when the recording started. We discuss this in the context of data collection because in the past sometimes we have given this task to local research assistants and they have provided the parents with a recording device and asked them to start the device at some point in the following day. This means you cannot have a specific record of when the recording was started and so that’s tricky because very often we do need to know the time samp in order to be able to interpret. Let me give you an example: it may happen that the parents forget to turn on the device and they actually only turn it on 1pm when you are in a place where the sun sets at 6pm and this means you’ll get 5 hours of daytime and then the rest of your recording will technically be at nightime and so, if you see lower levels of talking at that time it could just be that the family recorded at this time. The other thing to bear in mind with the USB is that the family can turn it on and off at any point in time and sometimes we have found that the USB do seem to have multiple files for reasons we haven’t understood, which could be families turning them on and off but it could also be for technical reasons that are beyond anyone’s control, and you can’t be sure what is the scope of the recording itself. Ideally you would want to deliver the recording device and write down what the time is. So one thing that you could do perhaps is the following: if you must leave the USB with the family, you can leave them a watch or some kind of clock and have them keep a record of when they started and stopped the recording. So, when you have these two edges, you can for instance non only know when the recording took place and adjust it to your quantitative estimates but also check whether there are any gaps in the recording based on if you are used to produce multiple files you can put them all together and make sure that this adds up to the same and if not, have an idea of how much time is missing between each of the files. 19.4 Storage One last piece of advice, which is pretty obvious if you have done field work in the past: make sure you have several devices on which you can make encrypted copies of your data, in case one or two of them fail or are stolen. Foresee about 2G per audio recording, so if you plan on making 100 recordings, then you’re probably better off with a couple of hard drives with 1T of space in each. 19.5 Resources Example USB please note we have not tested this one! so do buy a sample one before you buy them in bulk. There may be others that are shipped to your location. Casillas, M. &amp; Cristia, A. (2019). A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings, Collabra. link A step-by-step guide to collecting and analyzing long-format speech environment (LFSE) recordings Appendix to that guide "],["multilingualsettings.html", "Chapter 20 Performance across languages &amp; for bilingual or multilingual settings 20.1 Resources", " Chapter 20 Performance across languages &amp; for bilingual or multilingual settings Many people asks us whether automated analyses are valid in bilingual or multilingual settings. It is important to bear in mind that no automated tool exists that classifies sections of the audio as the different languages, and I think it will take many years before such a tool is developed. So if in your research you just want to get an idea of overall quantities of speech by and around the child, without separating the languages, then in general there is no clear reason why accuracy would be any different from that in monolingual settings. That said, as explained in the 14 Video, there is considerable variation in performance across corpora from the same language and culture, and across languages and cultures, for reasons that are not entirely clear yet. So particularly if you are working in a multilingual or bilingual setting where one or more of the languages represented have not been the object of a study on the automated analysis’ accuracy, it is a good idea to try and do one. To find out more, see the Video 15. 20.1 Resources Cychosz, M., Villanueva, A., &amp; Weisleder, A. (2020). Efficient estimation of children’s language exposure in two bilingual communities. pdf "],["alternatives.html", "Chapter 21 Alternative / Additional tests 21.1 If you want alternative measures of interaction 21.2 If you want use a more established instrument 21.3 References", " Chapter 21 Alternative / Additional tests Some people have asked us: In order to measure language in young children, what else should or could I collect? This is a complex question to answer because it depends on what you are hoping to gain from these additional tests. We have compiled here ideas from a variety of sources, and we explain the reasoning for our recommendations, but this is definitely one point in which we think discussion would be most helpful. So don’t hesitate to post comments in the chat or reach out to us. One common scenario is that you are interested in what long-form recordings can tell you about the interaction between the child and others, but you are not sure you will be able to collect it (or perhaps not for your whole sample). You are then looking for some measure of interaction that is easier to collect, perhaps faster and with lower logistical costs. We provide some ideas in the “Alternative measures of interaction” below. Another common scenario is that you are not sure of using long-form recordings on their own, since it is not a very established measure yet. Therefore, you want to complement (or replace) them with something that your stakeholders will find easier to understand and accept. In this case, you will typically want to use a more established instrument – then the second section below is for you. 21.1 If you want alternative measures of interaction It is quite common to use interaction tasks in which the child and a caregiver are provided with a set of objects (age-appropriate toys or household objects) and asked to play together. Such tasks often last 5-10 minutes, and are video-recorded using a tripod-based camera. If doing so, we recommend piloting the task with members of the community, to get their impression of how comfortable they feel; and provide the data to your coders, to make sure that you can actually code it. Common errors include setting up the camera too far, so you cannot see or hear the participants well; or at an angle where you only see one of the participants; or becoming so fussy with the camera position that parents are hyperconscious of the task. Some researchers have adapted this procedure to collect data remotely, by asking parents to video record their interaction with the child using zoom or some other such tool. We do not know of a similar procedure for audio only, but it may be worth trying, particularly in areas in which data are limited and/or parents do not have access to webcams. Once you have this audio(video) data, you can annotate it like what was described in our other sections for the long-form audio recordings. If you decide to do full segmentation and annotation, the difficulty is lower but still estimate about x15 (i.e., 10 mins of video takes 150 mins to annotate). As discussed in the relevant video, you can also make some decisions that are relevant to your own purposes with a coarser granularity – in our previous discussion, we talked about a system where coders make decisions every 30 seconds. You could even have a coder watch the video and make high-level decisions at the video level, such as how fluid the conversation was on a scale of 1 to 5. To our knowledge, there is no work comparing measures of children and others’ behavior from usch 5-10-minute play sessions against long-form recordings, so we do not know to what extent one captures similar patterns or not. However, in the SEEDLingS study we have already discussed, they compared behaviors in 1-h video against long-form found considerable convergence in e.g. proportion of speech by different speaker types as well as divergence in aspects like how much people talked (with a lot more speech in the shorter video recording). For more information, see the resources. 21.2 If you want use a more established instrument A good starting point for this is the report by Fernald and colleagues published by the World Bank in 2009. They explain that assessments should meet a variety of desiderata. Among them is the fact that they are “psychometrically adequate, valid and reliable”. We have seen validity and reliability in Video 14 – we’ll add here that the situation is similar to that for LENA, in that typically the people that evaluate the validity and reliability of a tool need it to work (either because they sell the tool or they have already collected data with it). So from our point of view, often the evidence is gathered by people who have some form of a conflict of interest. In that report, they discuss instruments that are not too expensive, that are adaptable to a wide range of cultural settings, and that do not suffer from floor or ceiling effects. Floor effects emerge when the test is so hard for a given population that you cannot really measure individual differences or intervention effects because everyone is already at the lowest level they could score. Ceiling effects emerge when the items are so easy that everyone scores very well, similarly making it hard to measure variation across children or effects of an intervention. We can also recommend the work of the INTERGROWTH project, which was done multinationally. They developed their short test, which we also introduce below, after carefully sifting through 47 tests as explained in the Fernandes paper provided in Resources. We are going to distinguish between instruments that rely solely or primarily on parental report from those that are primarily based on direct observation. There are pros and cons to both of these: Parental report is great if you want to have an idea of how the child acts in general, in their natural environment. However, if the “right” response is obvious to the parent, they may provide responses that over-estimate the child’s development. Alternatively, if the parent doesn’t actually spend much time with the child or doesn’t really understand what is being asked, then you may end up with an underestimate of the child’s abilities. To give a specific example, if you are evaluating an intervention aimed at increasing parents’ sensitivity to the child, then parents may report improvements in vocabulary not because they happened, but because you did manage to change parents’ behavior and now they pay more attention to the child’s vocabulary. Direct observations, for their part, can avoid some biases (particularly if the people administering them do not know whether a child is part of a treatment or control group, or their prior test scores, etc.) However, one should be very careful to interpret results if children seem to be uncomfortable with the tasks. Additionally, these tests have been developed in specific settings, and so one should be careful to compare results across groups for whom the activities represented in the tests (e.g., picture pointing) may be more versus less relevant. 21.2.1 Children 0-3 years For children under three years of age, there are very few options. We will discuss the subset of assessments that have sections dedicated to language, even if they are not specifically thought to test language development only. 21.2.1.1 Bayley Scales of Infant Development (BSID) Arguably, this is historically the most commonly used assessment of infant development in the world. There was a 4th edition released in 2019 that includes an online training and a Digital delivery option. The digital delivery concerns the administrators (there is no digital material for the child). It has been adapted to many languages and cultures, and used in hundreds of studies. To our knowledge, however, there are very few independent evaluations of the reliability and validity of BSID. Some worries had been raised for BSID-III (even in American samples) as a screening tool – which is now outdated given the new edition. 21.2.1.2 Denver Developmental Screening Test (DDST) The DDST is a comprehensive test of children’s development, and can be used to assess development from birth through 5 years of age. The Denver II was released in 1992. 21.2.1.3 MacArthur Communicative Development Inventories (CDI) CDIs are parent report forms for assessing language and communication skills in infants and young children. Most of the items are words: parents will report which among a list of words the child says (or comprehends, depending on the instrument). It has been adapted to many languages. 21.2.1.4 Ages and Stages Questionnaires (ASQ) The ASQ is a parent report, and can be completed by parents alone or administered by a trained assessor, who helps the parent understand the questions and/or decides based on observation of the child. The subscales measure skills in Communication, Gross Motor, Fine Motor, Personal-Social and Problem-Solving (similar to cognitive) domains. The third edition was published in 2009 and allows data to be managed online. At the time of writing, they are collecting data for a 4th edition. 21.2.1.5 Evaluacion de Escala de Evaluacion del Desorrollo Psicomotor (EEDP) The EEDP is a Spanish-language screening test initially developed in Chile and widely used in Latin America. 21.2.1.6 The Guide for Monitoring Child Development This parent report assessment provides a method for developmental monitoring and early detection of developmental difficulties in children of low and middle income countries. 21.2.1.7 Oxford Neurodevelopment Assessment (OX-NDA) The INTERGROWTH group developed their test after carefully sifting through previous tests. The items for cognitive and language (Table 1 on p. 9 of the manual and following pages for explanation) use a combination of direct observation and parental questionnaire. It is very fast to administer, and to train people to do so, and it’s easier to use in multilingual settings due to the fact that questions to the parents are quite simple and there shouldn’t be much “lost in translation”. To our knowledge, validity and reliability has only been measured by the same team who developed the task. 21.2.2 Children aged 3-5 years For this age range, you can use tests directly on the child (rather than parental reports), and that are more specific to language 21.2.2.1 Peabody Picture Vocabulary Test (PPVT) The PPVT (is a test of “receptive language” or listening comprehension for the spoken world and has been used in many countries throughout the world. The traditional version is a booklet with four images per page; the assessor then says the name of one of the images and the child needs to point to it. The forth edition can be administered on tablets, allowing online handling of data. 21.2.2.2 Reynell Developmental Language Scale The 134-item Reynell scale (Reynell, 1990) is comprised of two subscales to assess both Receptive Language and Expressive Language. The kit includes paper material. Online you can find webinars for people who assess the test. 21.2.3 Additional tests If you would like to use instruments in combination with LENA and want to be informed by previous work, there exists a meta-analysis of how LENA measures related to standardized instruments; this table contains the full list of papers with LENA + some test that they included in their analysis. If working with an American sample, specifically, the Preschool Language Scales (0-7 years) was used in the very large sample by Gilkerson et al. 2017, and has one of the highest and most precise correlations with CVC (though r = .38 – see forest plot). It can be used with children 0-6 years. Another commonly used test elsewhere is the Wechsler Preschool and Primary Scale of Intelligence (2-7 years), which contains a verbal component. 21.3 References Bergelson, E., Amatuni, A., Dailey, S., Koorathota, S., &amp; Tor, S. (2019). Day by day, hour by hour: Naturalistic language input to infants. Developmental science, 22(1), e12715. Fernald, L. C., Kariger, P., Engle, P., &amp; Raikes, A. (2009). Examining early child development in low-income countries. pdf Fernandes, M., Stein, A., Newton, C. R., Cheikh-Ismail, L., Kihara, M., Wulff, K., … &amp; International Fetal and Newborn Growth Consortium for the 21st Century (INTERGROWTH-21st). (2014). The INTERGROWTH-21st Project Neurodevelopment Package: a novel method for the multi-dimensional assessment of neurodevelopment in pre-school age children. PloS one, 9(11), e113360. Bayleys BSID CDI ASQ PPVT Reynell InterNDA Manual "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
