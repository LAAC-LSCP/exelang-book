[["automated-and-manual-annotations.html", "A Automated and manual annotations A.1 Sampling strategy A.2 Evaluating automated annotations from manual annotations", " A Automated and manual annotations A.1 Sampling strategy Human annotations require to define a sampling process, i.e. an algorithm to decide which portions of audio should be annotated. This raises a few questions, among which: How much audio should be annotated ? How should the clips be selected ? What should be the length of each clip ? Of course, the answer to each of this question depends on your research goals, and ((???)) provides clues to some of them. Most often, the amount of audio that can be annotated is heavily constrained by the availability of human resources, and can be considered fixed. For many tasks, the clips should be selected in a way that does not induce biases in the results. For instance, if one plans to estimate the quantity of speech produced or heard by the child, then it is important to use unbiased estimators. In order to evaluate the performance of automated tools, it might also be preferrable to use unbiased annotations which can assess the behavior of the algorithms in a variety of contexts rather than for a subset of configurations. We can cite at least two antagonistic ways of achieving such an unbiased sampling: By choosing each clip at random, independently from past choices By sampling the audio periodically (so that the position of the first clip completely determines the positions of subsequent clips) This does not settle the question of the length of each clip; the answer, again, will depend on the research question. For instance, exploring conversational behaviors may require longer clips than only assessing the amount of speech. Below, we show a way in which the choice of the sampling strategy may have a significant impact. We used automated annotations derived from 500 recordings from the Bergelson dataset (Bergelson 2015). Automated annotations are easy to produce in large amounts; this allows us to simulate a sampling method using semi-realistic distributions of speech. For a constant annotation budget of 20 minutes per recording, we estimated the total amount of speech for each recording using only annotations extracted with two sampling strategies - random clips vs periodic (i.e. evenly spaced) clips - and different lengths of clips. This allowed us to estimate the relative difference between the quantities of speech estimated from 20 minutes of annotations and the true quantities. The results are shown below. Figure A.1: Performance of several sampling strategies. (500 recordings; constant annotation budget of 20 minutes per recording.) It can be seen that the estimations are much more accurate when using many short clips rather than a few longer clips. The periodic sampling also yields more accurate results. This result is rather intuitive: annotating a portion of audio in close temporal proximity with a previous portion will yield much less information, because the two portions are correlated. Using annotations that are farther from each other helps probe a wider variety of configurations and thus yields better speech quantity estimates. This does not mean, of course, that one should annotate arbitrarily short clips. First, in practice, two 1-minute clip will take longer to annotate than one 2-minute clips, especially if annotators needs to listen to the context to make better decisions. Reducing the length of the clips may thus rapidly decrease the efficiency of the annotation process. Moreover, some estimators, such as vocalization counts, may be strongly biased by using short clips, due to the increased significance of boundary effects. These results emphasize the importance of carefully assessing the impact of the sampling strategy before engaging humans into time-consuming annotation efforts; A.2 Evaluating automated annotations from manual annotations Manual annotations may be used to assess the reliability of automated tools on a subset of a corpus. There are many ways to quantify the quality of automated annotations from a gold standard established by expert annotators. Typically, classification algorithms are evaluated using metrics such as Recall, Precision, or F-score. Many metrics more specific to certain speech processing tasks have been developed, some of which can be easily be calculated in python using the pyannote-metrics package (Bredin 2017). Agreement coefficients commonly used in Psychology and Natural Language Processing (Krippendorff' Alpha, Fleiss' Kappa, etc.) can also be used to evaluate the reliability of automated annotations. Another possibility is to calculate Pearson correlation coefficients between the metrics derived from the algorithm (e.g. vocalization counts or speech duration) and the ground truth for each of the sampled clips. To simulate the impact of the choice of the clip duration over these correlation coefficients, we again used the Bergelson dataset; we calculated the correlations between the vocalization counts and speech durations for the children and the adults, using both the LENA and the VTC annotations. Figure A.2: Pearson correlation between LENA and VTC derived metrics as a function of clips length. (500 recordings; periodic sampling; constant annotation budget of 60 minutes per recording.) We can observe a clear increase in the correlation coefficients as the length of the clips increases. This means Pearson correlation coefficients for these metrics are not robust again the length of the clips used to evalute the algorithms. One may turn to other metrics such as Recall and Precision, which remain stable for varying clip lengths. "]]
